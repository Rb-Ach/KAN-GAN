{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9715a90-f39c-4964-9193-5ee44431250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import kan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77d11873-35d1-466e-af7e-424c5607439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/S&P500.csv\n",
      "shape =  (5031, 7)\n"
     ]
    }
   ],
   "source": [
    "data_names = [\"S&P500\",\"SSE\",\"IBM\",\"MSFT\",\"PAICC\"]\n",
    "data_name = data_names[0]\n",
    "data_path=\"./datasets/S&P500.csv\"\n",
    "print(data_path)\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataframe.describe()\n",
    "print(\"shape = \",dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a805dde-5d75-41f4-8fe8-8796e074b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1999-01-11</td>\n",
       "      <td>1275.089966</td>\n",
       "      <td>1276.219971</td>\n",
       "      <td>1253.339966</td>\n",
       "      <td>1263.880005</td>\n",
       "      <td>1263.880005</td>\n",
       "      <td>818000000</td>\n",
       "      <td>1258.007983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1999-01-12</td>\n",
       "      <td>1263.880005</td>\n",
       "      <td>1264.449951</td>\n",
       "      <td>1238.290039</td>\n",
       "      <td>1239.510010</td>\n",
       "      <td>1239.510010</td>\n",
       "      <td>800200000</td>\n",
       "      <td>1265.163989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1999-01-13</td>\n",
       "      <td>1239.510010</td>\n",
       "      <td>1247.750000</td>\n",
       "      <td>1205.459961</td>\n",
       "      <td>1234.400024</td>\n",
       "      <td>1234.400024</td>\n",
       "      <td>931500000</td>\n",
       "      <td>1264.109985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999-01-14</td>\n",
       "      <td>1234.400024</td>\n",
       "      <td>1236.810059</td>\n",
       "      <td>1209.540039</td>\n",
       "      <td>1212.189941</td>\n",
       "      <td>1212.189941</td>\n",
       "      <td>797200000</td>\n",
       "      <td>1256.521997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1999-01-15</td>\n",
       "      <td>1212.189941</td>\n",
       "      <td>1243.260010</td>\n",
       "      <td>1212.189941</td>\n",
       "      <td>1243.260010</td>\n",
       "      <td>1243.260010</td>\n",
       "      <td>798100000</td>\n",
       "      <td>1245.013989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "5  1999-01-11  1275.089966  1276.219971  1253.339966  1263.880005   \n",
       "6  1999-01-12  1263.880005  1264.449951  1238.290039  1239.510010   \n",
       "7  1999-01-13  1239.510010  1247.750000  1205.459961  1234.400024   \n",
       "8  1999-01-14  1234.400024  1236.810059  1209.540039  1212.189941   \n",
       "9  1999-01-15  1212.189941  1243.260010  1212.189941  1243.260010   \n",
       "\n",
       "     Adj Close     Volume           Ma  \n",
       "5  1263.880005  818000000  1258.007983  \n",
       "6  1239.510010  800200000  1265.163989  \n",
       "7  1234.400024  931500000  1264.109985  \n",
       "8  1212.189941  797200000  1256.521997  \n",
       "9  1243.260010  798100000  1245.013989  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_Ma(dataframe):\n",
    "  Ma_window=5\n",
    "  for i in range(0,dataframe.shape[0]-Ma_window):\n",
    "    dataframe.loc[dataframe.index[i+Ma_window],'Ma'] = np.round(((dataframe.iloc[i,4]+ dataframe.iloc[i+1,4] +dataframe.iloc[i+2,4] + dataframe.iloc[i+3,4]+ dataframe.iloc[i+4,4])/5),6)\n",
    "  return dataframe[5:-5]\n",
    "\n",
    "dataframe=add_Ma(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7331a555-0b19-4c1e-8b75-f2323f80746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torchsummary import summary\n",
    "\n",
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(self, n_sequence, n_features):\n",
    "        super(GeneratorModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=n_features, hidden_size=10, batch_first=True)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(10)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.3)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(input_size=10, hidden_size=10, batch_first=True)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(10)  \n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.output_dense = nn.Linear(10, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.batch_norm1(x.permute(0, 2, 1)).permute(0, 2, 1) \n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        _, (x, _) = self.lstm2(x)\n",
    "        x=x.permute(1, 0, 2)\n",
    "        x = self.batch_norm2(x.permute(0, 2, 1)).permute(0, 2, 1)  \n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.output_dense(x)\n",
    "        x = self.leaky_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "n_sequence = 5  # Sequence length\n",
    "n_features = 7   # Number of features\n",
    "\n",
    "class DiscriminatorModel(nn.Module):\n",
    "    def __init__(self, n_sequence, n_features):\n",
    "        super(DiscriminatorModel, self).__init__()\n",
    "        input_dim = (n_sequence + 1) * n_features\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 72),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(72, 100),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(100, 10),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "generator = GeneratorModel(n_sequence, n_features).to(torch.float64)\n",
    "discriminator = DiscriminatorModel(n_sequence,n_features).to(torch.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "530f7259-5fdf-4575-ae9f-374cb66617e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "class Standarized_TimeseriesGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, length, batch_size=1, stride=1):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        samples = [self.data[i:i+self.length] for i in range(index, len(self.data)-self.length, self.stride)]\n",
    "        targets = [self.targets[i+self.length] for i in range(index, len(self.data)-self.length, self.stride)]\n",
    "        # Pack sequences into tensor\n",
    "        samples = torch.tensor(samples[0])\n",
    "        targets = torch.tensor(targets[0])\n",
    "        samples= samples.to(torch.float64)\n",
    "        targets= targets.to(torch.float64)\n",
    "        # shape : (n_batch, n_sequence, n_features)\n",
    "        mean = samples.mean(dim=0)\n",
    "        std = samples.std(dim=0,correction=0)\n",
    "        samples = (samples - mean)/std  # standardize along each feature\n",
    "\n",
    "\n",
    "        # targets = (targets - mean[..., 3])/std[..., 3]  # The close value is our target\n",
    "        targets = (targets - mean)/std  # The close value is our target\n",
    "        return samples, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa1adc1c-1b77-4f26-9359-68d3737a25b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4)\n",
      "[[0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 1.000e+00 1.000e+00]\n",
      " [2.000e+00 4.000e+00 8.000e+00 1.600e+01]\n",
      " [3.000e+00 9.000e+00 2.700e+01 8.100e+01]\n",
      " [4.000e+00 1.600e+01 6.400e+01 2.560e+02]\n",
      " [5.000e+00 2.500e+01 1.250e+02 6.250e+02]\n",
      " [6.000e+00 3.600e+01 2.160e+02 1.296e+03]\n",
      " [7.000e+00 4.900e+01 3.430e+02 2.401e+03]\n",
      " [8.000e+00 6.400e+01 5.120e+02 4.096e+03]\n",
      " [9.000e+00 8.100e+01 7.290e+02 6.561e+03]]\n",
      "tensor([[   10.00000000,   100.00000000,  1000.00000000, 10000.00000000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "data = np.array([[i, i**2,i**3,i**4] for i in range(11)])\n",
    "targets = data\n",
    "print(targets.shape)\n",
    "\n",
    "# Print shape of targets\n",
    "# Calculate mean and std for data\n",
    "mean = data[:-1].mean(axis=0)[None,:]\n",
    "std = data[:-1].std(axis=0)[None,:]\n",
    "# Create Standarized_TimeseriesGenerator instance\n",
    "data_gen = Standarized_TimeseriesGenerator(data, targets,\n",
    "                               length=10, batch_size=2, stride=1)\n",
    "# Retrieve first batch\n",
    "batch_0 = data_gen[0]\n",
    "x, y = batch_0\n",
    "print(np.array(x*std+mean))\n",
    "print(y*std+mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b687e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequence = 5\n",
    "n_features = 7\n",
    "n_batch = 32\n",
    "\n",
    "def get_gen_train_test(dataframe):\n",
    "  data = dataframe.drop(columns='Date').to_numpy()\n",
    "  #targets = data[:,3, None] #add none to have same number of dimensions as data\n",
    "  targets = data\n",
    "  n_samples = data.shape[0]\n",
    "  train_test_split=int(n_samples*0.9)\n",
    "  train_data = data[:train_test_split]\n",
    "  test_data = data[train_test_split:]\n",
    "  train_target = targets[:train_test_split]\n",
    "  test_target = targets[train_test_split:]\n",
    "  data_train = Standarized_TimeseriesGenerator(train_data, train_target,\n",
    "                                length=n_sequence, \n",
    "                                stride=1)\n",
    "  data_test = Standarized_TimeseriesGenerator(test_data, test_target,\n",
    "                                length=n_sequence, \n",
    "                                stride=1)\n",
    "  \n",
    "  train_loader = DataLoader(data_train, batch_size=n_batch, shuffle=True)\n",
    "  test_loader = DataLoader(data_test, batch_size=n_batch, shuffle=False)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "data_gen_train, data_gen_test = get_gen_train_test(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c63ba690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.33226763e-15,  3.33066907e-15, -2.44249065e-15, -1.33226763e-15,\n",
      "          -1.33226763e-15,  0.00000000e+00,  5.20417043e-16],\n",
      "         [-8.88178420e-16,  1.66533454e-15, -1.22124533e-15, -4.16333634e-17,\n",
      "          -4.16333634e-17,  0.00000000e+00,  1.57651669e-14],\n",
      "         [ 2.77555756e-16, -8.88178420e-16,  1.55431223e-15,  2.22044605e-16,\n",
      "           2.22044605e-16,  0.00000000e+00,  1.35447209e-14],\n",
      "         [ 4.99600361e-16, -2.66453526e-15,  1.22124533e-15,  1.33226763e-15,\n",
      "           1.33226763e-15,  0.00000000e+00, -2.63677968e-15],\n",
      "         [ 1.55431223e-15, -1.66533454e-15,  9.99200722e-16, -2.22044605e-16,\n",
      "          -2.22044605e-16,  0.00000000e+00, -2.70894418e-14]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[ 8.32667268e-17, -6.24500451e-17, -8.88178420e-16, -7.77156117e-16,\n",
      "         -7.77156117e-16,  0.00000000e+00, -4.04121181e-14]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([5, 7])\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "# test on data\n",
    "data = dataframe.drop(columns='Date').to_numpy()\n",
    "targets = data\n",
    "\n",
    "x_gen, y_gen = Standarized_TimeseriesGenerator(data, targets,\n",
    "                               length=5, \n",
    "                               stride=1, batch_size=1)[0]\n",
    "\n",
    "\n",
    "x = data[None, :5,:]\n",
    "mean = x.mean(axis=1)\n",
    "std = x.std(axis=1)\n",
    "x = (x - mean)/std\n",
    "y = (data[5] - mean)/std\n",
    "print(torch.tensor(x)-x_gen)\n",
    "print(torch.tensor(y)-y_gen)\n",
    "print(x_gen.shape)\n",
    "print(y_gen.reshape(1,7).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3a41498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a1 = 0.01\n",
    "a2 = 0.99\n",
    "learning_rate1 = 0.0001\n",
    "learning_rate2 = 0.0001\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "mse_fn = nn.MSELoss()\n",
    "\n",
    "def descriminator_loss(real_output, fake_output):\n",
    "    real_loss = loss_fn(real_output, torch.ones_like(real_output))\n",
    "    fake_loss = loss_fn(fake_output, torch.zeros_like(fake_output))\n",
    "    return real_loss + fake_loss\n",
    "def generator_loss(x,y,fake_output):\n",
    "    loss = loss_fn(fake_output, torch.ones_like(fake_output))\n",
    "    mse_loss = mse_fn(x.reshape(-1,1,7),y.reshape(-1,1,7))\n",
    "    return a1*mse_loss + a2*loss , mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de38861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][141/142]\tLoss_D: 1.3792\tLoss_G: 0.7976 \tMSE_loss: 4.7828 \tTime: 0.0490\n",
      "Validation \tLoss_D: 1.3747\tLoss_G: 0.8021 \tMSE_loss: 5.5139 \tBest_loss: 5.5139\n",
      "[1/200][141/142]\tLoss_D: 1.3691\tLoss_G: 0.7673 \tMSE_loss: 4.8526 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.3615\tLoss_G: 0.7864 \tMSE_loss: 5.4582 \tBest_loss: 5.4582\n",
      "[2/200][141/142]\tLoss_D: 1.3611\tLoss_G: 0.7539 \tMSE_loss: 3.4702 \tTime: 0.0438\n",
      "Validation \tLoss_D: 1.3464\tLoss_G: 0.7703 \tMSE_loss: 5.4213 \tBest_loss: 5.4213\n",
      "[3/200][141/142]\tLoss_D: 1.3324\tLoss_G: 0.7657 \tMSE_loss: 5.1758 \tTime: 0.0525\n",
      "Validation \tLoss_D: 1.3291\tLoss_G: 0.7605 \tMSE_loss: 5.3634 \tBest_loss: 5.3634\n",
      "[4/200][141/142]\tLoss_D: 1.3107\tLoss_G: 0.7570 \tMSE_loss: 5.8688 \tTime: 0.0456\n",
      "Validation \tLoss_D: 1.3062\tLoss_G: 0.7542 \tMSE_loss: 5.3105 \tBest_loss: 5.3105\n",
      "[5/200][141/142]\tLoss_D: 1.3237\tLoss_G: 0.7276 \tMSE_loss: 3.3354 \tTime: 0.0412\n",
      "Validation \tLoss_D: 1.2789\tLoss_G: 0.7512 \tMSE_loss: 5.2312 \tBest_loss: 5.2312\n",
      "[6/200][141/142]\tLoss_D: 1.2702\tLoss_G: 0.7669 \tMSE_loss: 5.8299 \tTime: 0.0421\n",
      "Validation \tLoss_D: 1.2450\tLoss_G: 0.7566 \tMSE_loss: 5.1584 \tBest_loss: 5.1584\n",
      "[7/200][141/142]\tLoss_D: 1.2839\tLoss_G: 0.7572 \tMSE_loss: 4.2263 \tTime: 0.0520\n",
      "Validation \tLoss_D: 1.2019\tLoss_G: 0.7755 \tMSE_loss: 5.1088 \tBest_loss: 5.1088\n",
      "[8/200][141/142]\tLoss_D: 1.1050\tLoss_G: 0.7913 \tMSE_loss: 5.3286 \tTime: 0.0474\n",
      "Validation \tLoss_D: 1.1593\tLoss_G: 0.7916 \tMSE_loss: 5.0193 \tBest_loss: 5.0193\n",
      "[9/200][141/142]\tLoss_D: 1.1450\tLoss_G: 0.8262 \tMSE_loss: 4.2826 \tTime: 0.0465\n",
      "Validation \tLoss_D: 1.1123\tLoss_G: 0.8228 \tMSE_loss: 4.9462 \tBest_loss: 4.9462\n",
      "[10/200][141/142]\tLoss_D: 1.1044\tLoss_G: 0.8770 \tMSE_loss: 2.6276 \tTime: 0.0445\n",
      "Validation \tLoss_D: 1.0561\tLoss_G: 0.8743 \tMSE_loss: 4.9101 \tBest_loss: 4.9101\n",
      "[11/200][141/142]\tLoss_D: 0.9766\tLoss_G: 1.0214 \tMSE_loss: 7.1001 \tTime: 0.0497\n",
      "Validation \tLoss_D: 1.0106\tLoss_G: 0.9221 \tMSE_loss: 4.8482 \tBest_loss: 4.8482\n",
      "[12/200][141/142]\tLoss_D: 1.0747\tLoss_G: 1.0125 \tMSE_loss: 3.8551 \tTime: 0.0430\n",
      "Validation \tLoss_D: 0.9781\tLoss_G: 0.9615 \tMSE_loss: 4.7698 \tBest_loss: 4.7698\n",
      "[13/200][141/142]\tLoss_D: 0.9862\tLoss_G: 1.1287 \tMSE_loss: 3.9097 \tTime: 0.0516\n",
      "Validation \tLoss_D: 0.9472\tLoss_G: 1.0022 \tMSE_loss: 4.7255 \tBest_loss: 4.7255\n",
      "[14/200][141/142]\tLoss_D: 1.0309\tLoss_G: 1.1361 \tMSE_loss: 5.3624 \tTime: 0.0463\n",
      "Validation \tLoss_D: 0.9012\tLoss_G: 1.0722 \tMSE_loss: 4.6968 \tBest_loss: 4.6968\n",
      "[15/200][141/142]\tLoss_D: 0.8713\tLoss_G: 1.0169 \tMSE_loss: 6.2663 \tTime: 0.0505\n",
      "Validation \tLoss_D: 0.8817\tLoss_G: 1.1020 \tMSE_loss: 4.6518 \tBest_loss: 4.6518\n",
      "[16/200][141/142]\tLoss_D: 0.8863\tLoss_G: 1.1802 \tMSE_loss: 4.7614 \tTime: 0.0473\n",
      "Validation \tLoss_D: 0.8617\tLoss_G: 1.1429 \tMSE_loss: 4.6273 \tBest_loss: 4.6273\n",
      "[17/200][141/142]\tLoss_D: 0.7861\tLoss_G: 1.1640 \tMSE_loss: 4.1429 \tTime: 0.0434\n",
      "Validation \tLoss_D: 0.8490\tLoss_G: 1.1673 \tMSE_loss: 4.5986 \tBest_loss: 4.5986\n",
      "[18/200][141/142]\tLoss_D: 0.8656\tLoss_G: 1.2666 \tMSE_loss: 3.0867 \tTime: 0.0474\n",
      "Validation \tLoss_D: 0.8365\tLoss_G: 1.2058 \tMSE_loss: 4.5783 \tBest_loss: 4.5783\n",
      "[19/200][141/142]\tLoss_D: 0.8881\tLoss_G: 1.1623 \tMSE_loss: 4.0829 \tTime: 0.0437\n",
      "Validation \tLoss_D: 0.8177\tLoss_G: 1.2465 \tMSE_loss: 4.5584 \tBest_loss: 4.5584\n",
      "[20/200][141/142]\tLoss_D: 0.8904\tLoss_G: 1.2431 \tMSE_loss: 4.7594 \tTime: 0.0477\n",
      "Validation \tLoss_D: 0.8004\tLoss_G: 1.2777 \tMSE_loss: 4.5294 \tBest_loss: 4.5294\n",
      "[21/200][141/142]\tLoss_D: 0.8983\tLoss_G: 1.0791 \tMSE_loss: 4.1324 \tTime: 0.0445\n",
      "Validation \tLoss_D: 0.8026\tLoss_G: 1.2855 \tMSE_loss: 4.5148 \tBest_loss: 4.5148\n",
      "[22/200][141/142]\tLoss_D: 0.7359\tLoss_G: 1.2332 \tMSE_loss: 4.1179 \tTime: 0.0464\n",
      "Validation \tLoss_D: 0.8010\tLoss_G: 1.2980 \tMSE_loss: 4.5020 \tBest_loss: 4.5020\n",
      "[23/200][141/142]\tLoss_D: 0.6937\tLoss_G: 1.3744 \tMSE_loss: 3.4086 \tTime: 0.0450\n",
      "Validation \tLoss_D: 0.7919\tLoss_G: 1.3205 \tMSE_loss: 4.4908 \tBest_loss: 4.4908\n",
      "[24/200][141/142]\tLoss_D: 0.6978\tLoss_G: 1.3182 \tMSE_loss: 4.5603 \tTime: 0.0470\n",
      "Validation \tLoss_D: 0.7870\tLoss_G: 1.3588 \tMSE_loss: 4.4641 \tBest_loss: 4.4641\n",
      "[25/200][141/142]\tLoss_D: 0.9635\tLoss_G: 1.3077 \tMSE_loss: 5.0297 \tTime: 0.0485\n",
      "Validation \tLoss_D: 0.8038\tLoss_G: 1.3575 \tMSE_loss: 4.4903 \tBest_loss: 4.4641\n",
      "[26/200][141/142]\tLoss_D: 0.7721\tLoss_G: 1.6332 \tMSE_loss: 3.1068 \tTime: 0.0440\n",
      "Validation \tLoss_D: 0.7835\tLoss_G: 1.3849 \tMSE_loss: 4.4314 \tBest_loss: 4.4314\n",
      "[27/200][141/142]\tLoss_D: 0.9357\tLoss_G: 1.4056 \tMSE_loss: 4.8513 \tTime: 0.0479\n",
      "Validation \tLoss_D: 0.7898\tLoss_G: 1.3858 \tMSE_loss: 4.4284 \tBest_loss: 4.4284\n",
      "[28/200][141/142]\tLoss_D: 0.8844\tLoss_G: 1.5544 \tMSE_loss: 4.2885 \tTime: 0.0500\n",
      "Validation \tLoss_D: 0.7818\tLoss_G: 1.3991 \tMSE_loss: 4.4075 \tBest_loss: 4.4075\n",
      "[29/200][141/142]\tLoss_D: 0.7892\tLoss_G: 1.6047 \tMSE_loss: 6.0977 \tTime: 0.0504\n",
      "Validation \tLoss_D: 0.7921\tLoss_G: 1.4335 \tMSE_loss: 4.4050 \tBest_loss: 4.4050\n",
      "[30/200][141/142]\tLoss_D: 0.8929\tLoss_G: 1.2664 \tMSE_loss: 3.8221 \tTime: 0.0434\n",
      "Validation \tLoss_D: 0.7885\tLoss_G: 1.4324 \tMSE_loss: 4.3919 \tBest_loss: 4.3919\n",
      "[31/200][141/142]\tLoss_D: 0.7433\tLoss_G: 1.4960 \tMSE_loss: 4.7976 \tTime: 0.0445\n",
      "Validation \tLoss_D: 0.7849\tLoss_G: 1.4623 \tMSE_loss: 4.3965 \tBest_loss: 4.3919\n",
      "[32/200][141/142]\tLoss_D: 0.8588\tLoss_G: 1.5927 \tMSE_loss: 4.2252 \tTime: 0.0479\n",
      "Validation \tLoss_D: 0.8096\tLoss_G: 1.4479 \tMSE_loss: 4.4270 \tBest_loss: 4.3919\n",
      "[33/200][141/142]\tLoss_D: 1.0672\tLoss_G: 1.6029 \tMSE_loss: 2.7441 \tTime: 0.0467\n",
      "Validation \tLoss_D: 0.7968\tLoss_G: 1.4682 \tMSE_loss: 4.4099 \tBest_loss: 4.3919\n",
      "[34/200][141/142]\tLoss_D: 0.8221\tLoss_G: 1.6831 \tMSE_loss: 4.7243 \tTime: 0.0492\n",
      "Validation \tLoss_D: 0.7791\tLoss_G: 1.4913 \tMSE_loss: 4.3739 \tBest_loss: 4.3739\n",
      "[35/200][141/142]\tLoss_D: 0.7762\tLoss_G: 1.4742 \tMSE_loss: 3.5391 \tTime: 0.0459\n",
      "Validation \tLoss_D: 0.7885\tLoss_G: 1.4904 \tMSE_loss: 4.4054 \tBest_loss: 4.3739\n",
      "[36/200][141/142]\tLoss_D: 0.9137\tLoss_G: 1.8039 \tMSE_loss: 3.8090 \tTime: 0.0512\n",
      "Validation \tLoss_D: 0.7745\tLoss_G: 1.5253 \tMSE_loss: 4.3876 \tBest_loss: 4.3739\n",
      "[37/200][141/142]\tLoss_D: 0.8824\tLoss_G: 1.6770 \tMSE_loss: 4.8871 \tTime: 0.0437\n",
      "Validation \tLoss_D: 0.7788\tLoss_G: 1.5100 \tMSE_loss: 4.3730 \tBest_loss: 4.3730\n",
      "[38/200][141/142]\tLoss_D: 0.7053\tLoss_G: 1.6992 \tMSE_loss: 4.1372 \tTime: 0.0487\n",
      "Validation \tLoss_D: 0.7813\tLoss_G: 1.5184 \tMSE_loss: 4.4028 \tBest_loss: 4.3730\n",
      "[39/200][141/142]\tLoss_D: 0.7665\tLoss_G: 1.5606 \tMSE_loss: 4.1595 \tTime: 0.0454\n",
      "Validation \tLoss_D: 0.7627\tLoss_G: 1.5591 \tMSE_loss: 4.3805 \tBest_loss: 4.3730\n",
      "[40/200][141/142]\tLoss_D: 0.6794\tLoss_G: 1.7635 \tMSE_loss: 4.2539 \tTime: 0.0482\n",
      "Validation \tLoss_D: 0.7686\tLoss_G: 1.5485 \tMSE_loss: 4.4250 \tBest_loss: 4.3730\n",
      "[41/200][141/142]\tLoss_D: 0.7828\tLoss_G: 1.7036 \tMSE_loss: 3.3086 \tTime: 0.0517\n",
      "Validation \tLoss_D: 0.7592\tLoss_G: 1.5671 \tMSE_loss: 4.3982 \tBest_loss: 4.3730\n",
      "[42/200][141/142]\tLoss_D: 1.0121\tLoss_G: 1.4920 \tMSE_loss: 3.0145 \tTime: 0.0466\n",
      "Validation \tLoss_D: 0.7501\tLoss_G: 1.5763 \tMSE_loss: 4.3759 \tBest_loss: 4.3730\n",
      "[43/200][141/142]\tLoss_D: 0.7116\tLoss_G: 2.0111 \tMSE_loss: 6.3474 \tTime: 0.0435\n",
      "Validation \tLoss_D: 0.7510\tLoss_G: 1.5759 \tMSE_loss: 4.3928 \tBest_loss: 4.3730\n",
      "[44/200][141/142]\tLoss_D: 0.8414\tLoss_G: 1.5476 \tMSE_loss: 3.5148 \tTime: 0.0478\n",
      "Validation \tLoss_D: 0.7504\tLoss_G: 1.5812 \tMSE_loss: 4.4291 \tBest_loss: 4.3730\n",
      "[45/200][141/142]\tLoss_D: 0.7748\tLoss_G: 1.4102 \tMSE_loss: 3.8174 \tTime: 0.0439\n",
      "Validation \tLoss_D: 0.7452\tLoss_G: 1.5982 \tMSE_loss: 4.4265 \tBest_loss: 4.3730\n",
      "[46/200][141/142]\tLoss_D: 0.8194\tLoss_G: 1.5592 \tMSE_loss: 2.8901 \tTime: 0.0377\n",
      "Validation \tLoss_D: 0.7412\tLoss_G: 1.5856 \tMSE_loss: 4.4476 \tBest_loss: 4.3730\n",
      "[47/200][141/142]\tLoss_D: 0.9445\tLoss_G: 1.3614 \tMSE_loss: 4.6689 \tTime: 0.0402\n",
      "Validation \tLoss_D: 0.7396\tLoss_G: 1.5772 \tMSE_loss: 4.4739 \tBest_loss: 4.3730\n",
      "[48/200][141/142]\tLoss_D: 0.8370\tLoss_G: 1.4918 \tMSE_loss: 4.3165 \tTime: 0.0417\n",
      "Validation \tLoss_D: 0.7143\tLoss_G: 1.6438 \tMSE_loss: 4.4331 \tBest_loss: 4.3730\n",
      "[49/200][141/142]\tLoss_D: 0.8591\tLoss_G: 1.6100 \tMSE_loss: 4.6338 \tTime: 0.0433\n",
      "Validation \tLoss_D: 0.7240\tLoss_G: 1.6338 \tMSE_loss: 4.5266 \tBest_loss: 4.3730\n",
      "[50/200][141/142]\tLoss_D: 0.9190\tLoss_G: 1.5344 \tMSE_loss: 3.0397 \tTime: 0.0460\n",
      "Validation \tLoss_D: 0.7111\tLoss_G: 1.6369 \tMSE_loss: 4.5172 \tBest_loss: 4.3730\n",
      "[51/200][141/142]\tLoss_D: 0.8089\tLoss_G: 1.7185 \tMSE_loss: 3.9018 \tTime: 0.0403\n",
      "Validation \tLoss_D: 0.7199\tLoss_G: 1.6282 \tMSE_loss: 4.6217 \tBest_loss: 4.3730\n",
      "[52/200][141/142]\tLoss_D: 0.6917\tLoss_G: 1.6885 \tMSE_loss: 4.4779 \tTime: 0.0463\n",
      "Validation \tLoss_D: 0.7140\tLoss_G: 1.6325 \tMSE_loss: 4.6255 \tBest_loss: 4.3730\n",
      "[53/200][141/142]\tLoss_D: 0.9468\tLoss_G: 1.6553 \tMSE_loss: 3.2271 \tTime: 0.0452\n",
      "Validation \tLoss_D: 0.7210\tLoss_G: 1.6070 \tMSE_loss: 4.7238 \tBest_loss: 4.3730\n",
      "[54/200][141/142]\tLoss_D: 0.8228\tLoss_G: 1.5147 \tMSE_loss: 4.2201 \tTime: 0.0480\n",
      "Validation \tLoss_D: 0.7193\tLoss_G: 1.5981 \tMSE_loss: 4.7367 \tBest_loss: 4.3730\n",
      "[55/200][141/142]\tLoss_D: 0.8867\tLoss_G: 1.7069 \tMSE_loss: 3.4383 \tTime: 0.0465\n",
      "Validation \tLoss_D: 0.7170\tLoss_G: 1.6317 \tMSE_loss: 4.8161 \tBest_loss: 4.3730\n",
      "[56/200][141/142]\tLoss_D: 0.9041\tLoss_G: 2.0591 \tMSE_loss: 3.5783 \tTime: 0.0432\n",
      "Validation \tLoss_D: 0.7144\tLoss_G: 1.6323 \tMSE_loss: 4.8050 \tBest_loss: 4.3730\n",
      "[57/200][141/142]\tLoss_D: 0.7422\tLoss_G: 1.7207 \tMSE_loss: 6.2747 \tTime: 0.0503\n",
      "Validation \tLoss_D: 0.7223\tLoss_G: 1.5914 \tMSE_loss: 4.9485 \tBest_loss: 4.3730\n",
      "[58/200][141/142]\tLoss_D: 0.6153\tLoss_G: 1.7262 \tMSE_loss: 4.0249 \tTime: 0.0464\n",
      "Validation \tLoss_D: 0.7127\tLoss_G: 1.6364 \tMSE_loss: 4.9451 \tBest_loss: 4.3730\n",
      "[59/200][141/142]\tLoss_D: 0.8131\tLoss_G: 1.8632 \tMSE_loss: 6.6233 \tTime: 0.0397\n",
      "Validation \tLoss_D: 0.7088\tLoss_G: 1.6153 \tMSE_loss: 4.8445 \tBest_loss: 4.3730\n",
      "[60/200][141/142]\tLoss_D: 1.0438\tLoss_G: 1.7646 \tMSE_loss: 4.9772 \tTime: 0.0505\n",
      "Validation \tLoss_D: 0.7138\tLoss_G: 1.6373 \tMSE_loss: 5.0195 \tBest_loss: 4.3730\n",
      "[61/200][141/142]\tLoss_D: 0.8020\tLoss_G: 1.3050 \tMSE_loss: 5.2282 \tTime: 0.0496\n",
      "Validation \tLoss_D: 0.6998\tLoss_G: 1.6486 \tMSE_loss: 4.8808 \tBest_loss: 4.3730\n",
      "[62/200][141/142]\tLoss_D: 0.8001\tLoss_G: 1.5789 \tMSE_loss: 4.6198 \tTime: 0.0441\n",
      "Validation \tLoss_D: 0.7020\tLoss_G: 1.6432 \tMSE_loss: 4.9098 \tBest_loss: 4.3730\n",
      "[63/200][141/142]\tLoss_D: 1.0135\tLoss_G: 1.3286 \tMSE_loss: 5.8065 \tTime: 0.0453\n",
      "Validation \tLoss_D: 0.7128\tLoss_G: 1.5838 \tMSE_loss: 4.9277 \tBest_loss: 4.3730\n",
      "[64/200][141/142]\tLoss_D: 0.9095\tLoss_G: 1.8065 \tMSE_loss: 4.7232 \tTime: 0.0514\n",
      "Validation \tLoss_D: 0.7018\tLoss_G: 1.6416 \tMSE_loss: 4.8874 \tBest_loss: 4.3730\n",
      "[65/200][141/142]\tLoss_D: 0.7597\tLoss_G: 1.5336 \tMSE_loss: 3.9551 \tTime: 0.0463\n",
      "Validation \tLoss_D: 0.7043\tLoss_G: 1.6290 \tMSE_loss: 4.7982 \tBest_loss: 4.3730\n",
      "[66/200][141/142]\tLoss_D: 0.8500\tLoss_G: 1.7416 \tMSE_loss: 4.0495 \tTime: 0.0405\n",
      "Validation \tLoss_D: 0.7096\tLoss_G: 1.6021 \tMSE_loss: 4.7885 \tBest_loss: 4.3730\n",
      "[67/200][141/142]\tLoss_D: 0.8596\tLoss_G: 1.3578 \tMSE_loss: 5.1056 \tTime: 0.0434\n",
      "Validation \tLoss_D: 0.7171\tLoss_G: 1.5997 \tMSE_loss: 4.8762 \tBest_loss: 4.3730\n",
      "[68/200][141/142]\tLoss_D: 0.8011\tLoss_G: 1.4084 \tMSE_loss: 3.7507 \tTime: 0.0403\n",
      "Validation \tLoss_D: 0.7177\tLoss_G: 1.6108 \tMSE_loss: 4.8070 \tBest_loss: 4.3730\n",
      "[69/200][141/142]\tLoss_D: 1.0612\tLoss_G: 1.8349 \tMSE_loss: 4.9353 \tTime: 0.0459\n",
      "Validation \tLoss_D: 0.7163\tLoss_G: 1.6548 \tMSE_loss: 4.7188 \tBest_loss: 4.3730\n",
      "[70/200][141/142]\tLoss_D: 0.7807\tLoss_G: 1.9233 \tMSE_loss: 5.3224 \tTime: 0.0531\n",
      "Validation \tLoss_D: 0.7232\tLoss_G: 1.6094 \tMSE_loss: 4.7252 \tBest_loss: 4.3730\n",
      "[71/200][141/142]\tLoss_D: 0.6626\tLoss_G: 1.6054 \tMSE_loss: 3.7161 \tTime: 0.0437\n",
      "Validation \tLoss_D: 0.7300\tLoss_G: 1.5811 \tMSE_loss: 4.6064 \tBest_loss: 4.3730\n",
      "[72/200][141/142]\tLoss_D: 0.8695\tLoss_G: 1.7171 \tMSE_loss: 6.6910 \tTime: 0.0445\n",
      "Validation \tLoss_D: 0.7305\tLoss_G: 1.5864 \tMSE_loss: 4.6835 \tBest_loss: 4.3730\n",
      "[73/200][141/142]\tLoss_D: 0.7527\tLoss_G: 1.8790 \tMSE_loss: 5.2800 \tTime: 0.0433\n",
      "Validation \tLoss_D: 0.7186\tLoss_G: 1.6533 \tMSE_loss: 4.7737 \tBest_loss: 4.3730\n",
      "[74/200][141/142]\tLoss_D: 0.7394\tLoss_G: 2.1129 \tMSE_loss: 7.0836 \tTime: 0.0447\n",
      "Validation \tLoss_D: 0.7239\tLoss_G: 1.6468 \tMSE_loss: 4.6872 \tBest_loss: 4.3730\n",
      "[75/200][141/142]\tLoss_D: 0.8703\tLoss_G: 1.5970 \tMSE_loss: 5.5763 \tTime: 0.0505\n",
      "Validation \tLoss_D: 0.7270\tLoss_G: 1.6112 \tMSE_loss: 4.6348 \tBest_loss: 4.3730\n",
      "[76/200][141/142]\tLoss_D: 0.9121\tLoss_G: 1.3458 \tMSE_loss: 3.9321 \tTime: 0.0499\n",
      "Validation \tLoss_D: 0.7238\tLoss_G: 1.6138 \tMSE_loss: 4.6644 \tBest_loss: 4.3730\n",
      "[77/200][141/142]\tLoss_D: 0.7114\tLoss_G: 1.8286 \tMSE_loss: 5.1077 \tTime: 0.0480\n",
      "Validation \tLoss_D: 0.7282\tLoss_G: 1.6126 \tMSE_loss: 4.5303 \tBest_loss: 4.3730\n",
      "[78/200][141/142]\tLoss_D: 1.1621\tLoss_G: 1.4137 \tMSE_loss: 3.7815 \tTime: 0.0491\n",
      "Validation \tLoss_D: 0.7244\tLoss_G: 1.6061 \tMSE_loss: 4.6158 \tBest_loss: 4.3730\n",
      "[79/200][141/142]\tLoss_D: 0.6510\tLoss_G: 1.6147 \tMSE_loss: 6.2669 \tTime: 0.0503\n",
      "Validation \tLoss_D: 0.7245\tLoss_G: 1.6280 \tMSE_loss: 4.6492 \tBest_loss: 4.3730\n",
      "[80/200][141/142]\tLoss_D: 0.9427\tLoss_G: 1.7434 \tMSE_loss: 7.2807 \tTime: 0.0458\n",
      "Validation \tLoss_D: 0.7226\tLoss_G: 1.6070 \tMSE_loss: 4.5044 \tBest_loss: 4.3730\n",
      "[81/200][141/142]\tLoss_D: 0.7522\tLoss_G: 2.0921 \tMSE_loss: 4.8708 \tTime: 0.0409\n",
      "Validation \tLoss_D: 0.7170\tLoss_G: 1.6348 \tMSE_loss: 4.6716 \tBest_loss: 4.3730\n",
      "[82/200][141/142]\tLoss_D: 0.8338\tLoss_G: 1.5205 \tMSE_loss: 5.1497 \tTime: 0.0460\n",
      "Validation \tLoss_D: 0.7185\tLoss_G: 1.6101 \tMSE_loss: 4.5691 \tBest_loss: 4.3730\n",
      "[83/200][141/142]\tLoss_D: 0.7367\tLoss_G: 1.5774 \tMSE_loss: 5.2917 \tTime: 0.0453\n",
      "Validation \tLoss_D: 0.7154\tLoss_G: 1.6214 \tMSE_loss: 4.5413 \tBest_loss: 4.3730\n",
      "[84/200][141/142]\tLoss_D: 0.8606\tLoss_G: 1.4104 \tMSE_loss: 5.4696 \tTime: 0.0503\n",
      "Validation \tLoss_D: 0.7205\tLoss_G: 1.5945 \tMSE_loss: 4.5160 \tBest_loss: 4.3730\n",
      "[85/200][141/142]\tLoss_D: 0.5517\tLoss_G: 2.3234 \tMSE_loss: 4.0038 \tTime: 0.0514\n",
      "Validation \tLoss_D: 0.7062\tLoss_G: 1.6564 \tMSE_loss: 4.5697 \tBest_loss: 4.3730\n",
      "[86/200][141/142]\tLoss_D: 0.7684\tLoss_G: 1.9670 \tMSE_loss: 5.5423 \tTime: 0.0543\n",
      "Validation \tLoss_D: 0.7027\tLoss_G: 1.6718 \tMSE_loss: 4.4738 \tBest_loss: 4.3730\n",
      "[87/200][141/142]\tLoss_D: 0.7090\tLoss_G: 1.5322 \tMSE_loss: 7.4757 \tTime: 0.0497\n",
      "Validation \tLoss_D: 0.7042\tLoss_G: 1.6531 \tMSE_loss: 4.5297 \tBest_loss: 4.3730\n",
      "[88/200][141/142]\tLoss_D: 0.7120\tLoss_G: 1.5914 \tMSE_loss: 3.9000 \tTime: 0.0479\n",
      "Validation \tLoss_D: 0.7057\tLoss_G: 1.6585 \tMSE_loss: 4.5310 \tBest_loss: 4.3730\n",
      "[89/200][141/142]\tLoss_D: 0.8614\tLoss_G: 1.5946 \tMSE_loss: 4.2583 \tTime: 0.0515\n",
      "Validation \tLoss_D: 0.7019\tLoss_G: 1.6737 \tMSE_loss: 4.5358 \tBest_loss: 4.3730\n",
      "[90/200][141/142]\tLoss_D: 0.9238\tLoss_G: 1.6629 \tMSE_loss: 3.6079 \tTime: 0.0488\n",
      "Validation \tLoss_D: 0.6925\tLoss_G: 1.7103 \tMSE_loss: 4.5873 \tBest_loss: 4.3730\n",
      "[91/200][141/142]\tLoss_D: 0.7497\tLoss_G: 1.8518 \tMSE_loss: 4.1081 \tTime: 0.0462\n",
      "Validation \tLoss_D: 0.6984\tLoss_G: 1.6507 \tMSE_loss: 4.4373 \tBest_loss: 4.3730\n",
      "[92/200][141/142]\tLoss_D: 0.6000\tLoss_G: 2.0282 \tMSE_loss: 4.9529 \tTime: 0.0461\n",
      "Validation \tLoss_D: 0.6975\tLoss_G: 1.6866 \tMSE_loss: 4.5483 \tBest_loss: 4.3730\n",
      "[93/200][141/142]\tLoss_D: 0.7396\tLoss_G: 1.8656 \tMSE_loss: 6.2276 \tTime: 0.0493\n",
      "Validation \tLoss_D: 0.6862\tLoss_G: 1.7237 \tMSE_loss: 4.5080 \tBest_loss: 4.3730\n",
      "[94/200][141/142]\tLoss_D: 0.8601\tLoss_G: 1.7569 \tMSE_loss: 4.9049 \tTime: 0.0551\n",
      "Validation \tLoss_D: 0.6866\tLoss_G: 1.6917 \tMSE_loss: 4.5989 \tBest_loss: 4.3730\n",
      "[95/200][141/142]\tLoss_D: 0.9478\tLoss_G: 1.5058 \tMSE_loss: 4.8939 \tTime: 0.0529\n",
      "Validation \tLoss_D: 0.6836\tLoss_G: 1.7160 \tMSE_loss: 4.5050 \tBest_loss: 4.3730\n",
      "[96/200][141/142]\tLoss_D: 0.7501\tLoss_G: 1.7370 \tMSE_loss: 4.8626 \tTime: 0.0450\n",
      "Validation \tLoss_D: 0.6869\tLoss_G: 1.7047 \tMSE_loss: 4.5540 \tBest_loss: 4.3730\n",
      "[97/200][141/142]\tLoss_D: 0.8413\tLoss_G: 2.2203 \tMSE_loss: 6.6965 \tTime: 0.0465\n",
      "Validation \tLoss_D: 0.6878\tLoss_G: 1.6926 \tMSE_loss: 4.4290 \tBest_loss: 4.3730\n",
      "[98/200][141/142]\tLoss_D: 0.6430\tLoss_G: 1.6479 \tMSE_loss: 3.6594 \tTime: 0.0578\n",
      "Validation \tLoss_D: 0.6864\tLoss_G: 1.7036 \tMSE_loss: 4.4512 \tBest_loss: 4.3730\n",
      "[99/200][141/142]\tLoss_D: 0.7972\tLoss_G: 1.6280 \tMSE_loss: 4.8013 \tTime: 0.0527\n",
      "Validation \tLoss_D: 0.6861\tLoss_G: 1.7367 \tMSE_loss: 4.5202 \tBest_loss: 4.3730\n",
      "[100/200][141/142]\tLoss_D: 0.9909\tLoss_G: 1.7093 \tMSE_loss: 5.2134 \tTime: 0.0496\n",
      "Validation \tLoss_D: 0.6826\tLoss_G: 1.7246 \tMSE_loss: 4.4340 \tBest_loss: 4.3730\n",
      "[101/200][141/142]\tLoss_D: 0.9551\tLoss_G: 1.4790 \tMSE_loss: 3.5660 \tTime: 0.0505\n",
      "Validation \tLoss_D: 0.6844\tLoss_G: 1.7176 \tMSE_loss: 4.4929 \tBest_loss: 4.3730\n",
      "[102/200][141/142]\tLoss_D: 0.6187\tLoss_G: 1.6662 \tMSE_loss: 4.9854 \tTime: 0.0452\n",
      "Validation \tLoss_D: 0.6839\tLoss_G: 1.7532 \tMSE_loss: 4.5113 \tBest_loss: 4.3730\n",
      "[103/200][141/142]\tLoss_D: 0.8610\tLoss_G: 1.7583 \tMSE_loss: 4.3179 \tTime: 0.0451\n",
      "Validation \tLoss_D: 0.6824\tLoss_G: 1.7476 \tMSE_loss: 4.4984 \tBest_loss: 4.3730\n",
      "[104/200][141/142]\tLoss_D: 0.6561\tLoss_G: 1.5393 \tMSE_loss: 5.1262 \tTime: 0.0443\n",
      "Validation \tLoss_D: 0.6813\tLoss_G: 1.7403 \tMSE_loss: 4.3693 \tBest_loss: 4.3693\n",
      "[105/200][141/142]\tLoss_D: 0.9154\tLoss_G: 1.3609 \tMSE_loss: 4.0298 \tTime: 0.0502\n",
      "Validation \tLoss_D: 0.6892\tLoss_G: 1.7581 \tMSE_loss: 4.4581 \tBest_loss: 4.3693\n",
      "[106/200][141/142]\tLoss_D: 0.6717\tLoss_G: 1.8304 \tMSE_loss: 5.8250 \tTime: 0.0488\n",
      "Validation \tLoss_D: 0.6893\tLoss_G: 1.7477 \tMSE_loss: 4.4657 \tBest_loss: 4.3693\n",
      "[107/200][141/142]\tLoss_D: 0.8116\tLoss_G: 1.9370 \tMSE_loss: 5.1565 \tTime: 0.0546\n",
      "Validation \tLoss_D: 0.6857\tLoss_G: 1.7629 \tMSE_loss: 4.4102 \tBest_loss: 4.3693\n",
      "[108/200][141/142]\tLoss_D: 0.7979\tLoss_G: 1.8062 \tMSE_loss: 7.2037 \tTime: 0.0484\n",
      "Validation \tLoss_D: 0.6865\tLoss_G: 1.7786 \tMSE_loss: 4.3892 \tBest_loss: 4.3693\n",
      "[109/200][141/142]\tLoss_D: 0.6726\tLoss_G: 2.0806 \tMSE_loss: 6.9225 \tTime: 0.0537\n",
      "Validation \tLoss_D: 0.6983\tLoss_G: 1.7397 \tMSE_loss: 4.3805 \tBest_loss: 4.3693\n",
      "[110/200][141/142]\tLoss_D: 0.6744\tLoss_G: 1.6884 \tMSE_loss: 5.9274 \tTime: 0.0438\n",
      "Validation \tLoss_D: 0.7048\tLoss_G: 1.7340 \tMSE_loss: 4.4280 \tBest_loss: 4.3693\n",
      "[111/200][141/142]\tLoss_D: 0.7161\tLoss_G: 1.9976 \tMSE_loss: 4.0207 \tTime: 0.0538\n",
      "Validation \tLoss_D: 0.7017\tLoss_G: 1.7720 \tMSE_loss: 4.4481 \tBest_loss: 4.3693\n",
      "[112/200][141/142]\tLoss_D: 0.6328\tLoss_G: 1.6023 \tMSE_loss: 5.3769 \tTime: 0.0467\n",
      "Validation \tLoss_D: 0.7046\tLoss_G: 1.7253 \tMSE_loss: 4.3267 \tBest_loss: 4.3267\n",
      "[113/200][141/142]\tLoss_D: 0.8948\tLoss_G: 1.5102 \tMSE_loss: 3.0377 \tTime: 0.0502\n",
      "Validation \tLoss_D: 0.7114\tLoss_G: 1.7431 \tMSE_loss: 4.4513 \tBest_loss: 4.3267\n",
      "[114/200][141/142]\tLoss_D: 0.9100\tLoss_G: 1.5509 \tMSE_loss: 3.5484 \tTime: 0.0491\n",
      "Validation \tLoss_D: 0.7256\tLoss_G: 1.6957 \tMSE_loss: 4.3778 \tBest_loss: 4.3267\n",
      "[115/200][141/142]\tLoss_D: 0.6451\tLoss_G: 1.4301 \tMSE_loss: 4.8952 \tTime: 0.0434\n",
      "Validation \tLoss_D: 0.7281\tLoss_G: 1.7048 \tMSE_loss: 4.3684 \tBest_loss: 4.3267\n",
      "[116/200][141/142]\tLoss_D: 0.7344\tLoss_G: 1.9287 \tMSE_loss: 6.3054 \tTime: 0.0434\n",
      "Validation \tLoss_D: 0.7413\tLoss_G: 1.6872 \tMSE_loss: 4.3575 \tBest_loss: 4.3267\n",
      "[117/200][141/142]\tLoss_D: 1.2085\tLoss_G: 1.3258 \tMSE_loss: 3.0378 \tTime: 0.0424\n",
      "Validation \tLoss_D: 0.7459\tLoss_G: 1.6869 \tMSE_loss: 4.3572 \tBest_loss: 4.3267\n",
      "[118/200][141/142]\tLoss_D: 0.8230\tLoss_G: 1.5146 \tMSE_loss: 4.9979 \tTime: 0.0493\n",
      "Validation \tLoss_D: 0.7533\tLoss_G: 1.7023 \tMSE_loss: 4.4733 \tBest_loss: 4.3267\n",
      "[119/200][141/142]\tLoss_D: 0.9035\tLoss_G: 1.5107 \tMSE_loss: 4.5790 \tTime: 0.0453\n",
      "Validation \tLoss_D: 0.7536\tLoss_G: 1.6753 \tMSE_loss: 4.3748 \tBest_loss: 4.3267\n",
      "[120/200][141/142]\tLoss_D: 0.8023\tLoss_G: 1.6696 \tMSE_loss: 3.8422 \tTime: 0.0487\n",
      "Validation \tLoss_D: 0.7661\tLoss_G: 1.6625 \tMSE_loss: 4.4466 \tBest_loss: 4.3267\n",
      "[121/200][141/142]\tLoss_D: 0.8980\tLoss_G: 1.7278 \tMSE_loss: 5.3807 \tTime: 0.0400\n",
      "Validation \tLoss_D: 0.7714\tLoss_G: 1.6654 \tMSE_loss: 4.4039 \tBest_loss: 4.3267\n",
      "[122/200][141/142]\tLoss_D: 0.8364\tLoss_G: 1.5443 \tMSE_loss: 5.1552 \tTime: 0.0492\n",
      "Validation \tLoss_D: 0.7810\tLoss_G: 1.6376 \tMSE_loss: 4.4156 \tBest_loss: 4.3267\n",
      "[123/200][141/142]\tLoss_D: 0.9567\tLoss_G: 1.6944 \tMSE_loss: 3.7400 \tTime: 0.0533\n",
      "Validation \tLoss_D: 0.7845\tLoss_G: 1.6246 \tMSE_loss: 4.3396 \tBest_loss: 4.3267\n",
      "[124/200][141/142]\tLoss_D: 0.8601\tLoss_G: 1.8223 \tMSE_loss: 5.3873 \tTime: 0.0458\n",
      "Validation \tLoss_D: 0.7930\tLoss_G: 1.6316 \tMSE_loss: 4.3558 \tBest_loss: 4.3267\n",
      "[125/200][141/142]\tLoss_D: 0.8973\tLoss_G: 1.8051 \tMSE_loss: 8.3396 \tTime: 0.0557\n",
      "Validation \tLoss_D: 0.7952\tLoss_G: 1.6245 \tMSE_loss: 4.3748 \tBest_loss: 4.3267\n",
      "[126/200][141/142]\tLoss_D: 1.0869\tLoss_G: 1.6253 \tMSE_loss: 4.1863 \tTime: 0.0441\n",
      "Validation \tLoss_D: 0.8027\tLoss_G: 1.6243 \tMSE_loss: 4.3014 \tBest_loss: 4.3014\n",
      "[127/200][141/142]\tLoss_D: 1.2164\tLoss_G: 1.4943 \tMSE_loss: 4.9007 \tTime: 0.0438\n",
      "Validation \tLoss_D: 0.8097\tLoss_G: 1.5933 \tMSE_loss: 4.2609 \tBest_loss: 4.2609\n",
      "[128/200][141/142]\tLoss_D: 0.9841\tLoss_G: 1.4727 \tMSE_loss: 5.0681 \tTime: 0.0533\n",
      "Validation \tLoss_D: 0.8201\tLoss_G: 1.5873 \tMSE_loss: 4.3114 \tBest_loss: 4.2609\n",
      "[129/200][141/142]\tLoss_D: 1.0331\tLoss_G: 1.7348 \tMSE_loss: 5.8863 \tTime: 0.0552\n",
      "Validation \tLoss_D: 0.8296\tLoss_G: 1.5634 \tMSE_loss: 4.2852 \tBest_loss: 4.2609\n",
      "[130/200][141/142]\tLoss_D: 0.7451\tLoss_G: 2.1045 \tMSE_loss: 7.2960 \tTime: 0.0557\n",
      "Validation \tLoss_D: 0.8428\tLoss_G: 1.5386 \tMSE_loss: 4.3093 \tBest_loss: 4.2609\n",
      "[131/200][141/142]\tLoss_D: 0.9609\tLoss_G: 1.5267 \tMSE_loss: 6.8277 \tTime: 0.0526\n",
      "Validation \tLoss_D: 0.8384\tLoss_G: 1.5513 \tMSE_loss: 4.3576 \tBest_loss: 4.2609\n",
      "[132/200][141/142]\tLoss_D: 1.1038\tLoss_G: 1.6299 \tMSE_loss: 3.9205 \tTime: 0.0522\n",
      "Validation \tLoss_D: 0.8489\tLoss_G: 1.5248 \tMSE_loss: 4.3049 \tBest_loss: 4.2609\n",
      "[133/200][141/142]\tLoss_D: 1.0643\tLoss_G: 1.4970 \tMSE_loss: 4.2460 \tTime: 0.0467\n",
      "Validation \tLoss_D: 0.8537\tLoss_G: 1.5344 \tMSE_loss: 4.4610 \tBest_loss: 4.2609\n",
      "[134/200][141/142]\tLoss_D: 0.9427\tLoss_G: 1.4388 \tMSE_loss: 4.1480 \tTime: 0.0476\n",
      "Validation \tLoss_D: 0.8571\tLoss_G: 1.5142 \tMSE_loss: 4.2846 \tBest_loss: 4.2609\n",
      "[135/200][141/142]\tLoss_D: 1.2587\tLoss_G: 1.3744 \tMSE_loss: 4.3935 \tTime: 0.0490\n",
      "Validation \tLoss_D: 0.8730\tLoss_G: 1.4597 \tMSE_loss: 4.2831 \tBest_loss: 4.2609\n",
      "[136/200][141/142]\tLoss_D: 1.4241\tLoss_G: 1.3536 \tMSE_loss: 5.3315 \tTime: 0.0486\n",
      "Validation \tLoss_D: 0.8763\tLoss_G: 1.4781 \tMSE_loss: 4.3612 \tBest_loss: 4.2609\n",
      "[137/200][141/142]\tLoss_D: 0.9200\tLoss_G: 1.5099 \tMSE_loss: 2.9882 \tTime: 0.0432\n",
      "Validation \tLoss_D: 0.8823\tLoss_G: 1.4582 \tMSE_loss: 4.3050 \tBest_loss: 4.2609\n",
      "[138/200][141/142]\tLoss_D: 0.9033\tLoss_G: 1.4305 \tMSE_loss: 6.3375 \tTime: 0.0477\n",
      "Validation \tLoss_D: 0.8832\tLoss_G: 1.4726 \tMSE_loss: 4.3380 \tBest_loss: 4.2609\n",
      "[139/200][141/142]\tLoss_D: 0.9598\tLoss_G: 1.3829 \tMSE_loss: 3.6030 \tTime: 0.0439\n",
      "Validation \tLoss_D: 0.8891\tLoss_G: 1.4619 \tMSE_loss: 4.3336 \tBest_loss: 4.2609\n",
      "[140/200][141/142]\tLoss_D: 1.0274\tLoss_G: 1.6785 \tMSE_loss: 5.3190 \tTime: 0.0505\n",
      "Validation \tLoss_D: 0.8894\tLoss_G: 1.4720 \tMSE_loss: 4.3980 \tBest_loss: 4.2609\n",
      "[141/200][141/142]\tLoss_D: 0.8553\tLoss_G: 1.3502 \tMSE_loss: 4.3948 \tTime: 0.0449\n",
      "Validation \tLoss_D: 0.9012\tLoss_G: 1.4225 \tMSE_loss: 4.2866 \tBest_loss: 4.2609\n",
      "[142/200][141/142]\tLoss_D: 1.0750\tLoss_G: 1.4185 \tMSE_loss: 4.2138 \tTime: 0.0462\n",
      "Validation \tLoss_D: 0.9072\tLoss_G: 1.4237 \tMSE_loss: 4.3171 \tBest_loss: 4.2609\n",
      "[143/200][141/142]\tLoss_D: 1.0400\tLoss_G: 1.4044 \tMSE_loss: 4.5643 \tTime: 0.0586\n",
      "Validation \tLoss_D: 0.9175\tLoss_G: 1.3987 \tMSE_loss: 4.2638 \tBest_loss: 4.2609\n",
      "[144/200][141/142]\tLoss_D: 1.2249\tLoss_G: 1.5370 \tMSE_loss: 5.6044 \tTime: 0.0461\n",
      "Validation \tLoss_D: 0.9254\tLoss_G: 1.3975 \tMSE_loss: 4.3722 \tBest_loss: 4.2609\n",
      "[145/200][141/142]\tLoss_D: 0.9066\tLoss_G: 1.3705 \tMSE_loss: 3.4428 \tTime: 0.0428\n",
      "Validation \tLoss_D: 0.9281\tLoss_G: 1.3951 \tMSE_loss: 4.3151 \tBest_loss: 4.2609\n",
      "[146/200][141/142]\tLoss_D: 0.9027\tLoss_G: 1.3441 \tMSE_loss: 3.6031 \tTime: 0.0463\n",
      "Validation \tLoss_D: 0.9294\tLoss_G: 1.4029 \tMSE_loss: 4.3463 \tBest_loss: 4.2609\n",
      "[147/200][141/142]\tLoss_D: 1.0477\tLoss_G: 1.4937 \tMSE_loss: 4.4005 \tTime: 0.0524\n",
      "Validation \tLoss_D: 0.9354\tLoss_G: 1.3834 \tMSE_loss: 4.3608 \tBest_loss: 4.2609\n",
      "[148/200][141/142]\tLoss_D: 0.8616\tLoss_G: 1.6007 \tMSE_loss: 6.2206 \tTime: 0.0469\n",
      "Validation \tLoss_D: 0.9422\tLoss_G: 1.3776 \tMSE_loss: 4.2775 \tBest_loss: 4.2609\n",
      "[149/200][141/142]\tLoss_D: 1.2308\tLoss_G: 1.5297 \tMSE_loss: 6.0713 \tTime: 0.0495\n",
      "Validation \tLoss_D: 0.9527\tLoss_G: 1.3436 \tMSE_loss: 4.2124 \tBest_loss: 4.2124\n",
      "[150/200][141/142]\tLoss_D: 0.9874\tLoss_G: 1.7024 \tMSE_loss: 4.5266 \tTime: 0.0499\n",
      "Validation \tLoss_D: 0.9505\tLoss_G: 1.3695 \tMSE_loss: 4.2437 \tBest_loss: 4.2124\n",
      "[151/200][141/142]\tLoss_D: 1.0288\tLoss_G: 1.4653 \tMSE_loss: 4.0320 \tTime: 0.0455\n",
      "Validation \tLoss_D: 0.9596\tLoss_G: 1.3466 \tMSE_loss: 4.3159 \tBest_loss: 4.2124\n",
      "[152/200][141/142]\tLoss_D: 1.0816\tLoss_G: 1.6192 \tMSE_loss: 4.7266 \tTime: 0.0477\n",
      "Validation \tLoss_D: 0.9705\tLoss_G: 1.3109 \tMSE_loss: 4.2693 \tBest_loss: 4.2124\n",
      "[153/200][141/142]\tLoss_D: 0.9922\tLoss_G: 1.2729 \tMSE_loss: 5.2983 \tTime: 0.0524\n",
      "Validation \tLoss_D: 0.9778\tLoss_G: 1.3152 \tMSE_loss: 4.2780 \tBest_loss: 4.2124\n",
      "[154/200][141/142]\tLoss_D: 1.1822\tLoss_G: 1.2210 \tMSE_loss: 12.4089 \tTime: 0.0511\n",
      "Validation \tLoss_D: 0.9870\tLoss_G: 1.2802 \tMSE_loss: 4.2334 \tBest_loss: 4.2124\n",
      "[155/200][141/142]\tLoss_D: 0.9980\tLoss_G: 1.4653 \tMSE_loss: 5.5327 \tTime: 0.0465\n",
      "Validation \tLoss_D: 0.9893\tLoss_G: 1.2762 \tMSE_loss: 4.2152 \tBest_loss: 4.2124\n",
      "[156/200][141/142]\tLoss_D: 1.1790\tLoss_G: 1.2194 \tMSE_loss: 3.9147 \tTime: 0.0528\n",
      "Validation \tLoss_D: 0.9839\tLoss_G: 1.2878 \tMSE_loss: 4.2156 \tBest_loss: 4.2124\n",
      "[157/200][141/142]\tLoss_D: 0.9242\tLoss_G: 1.3099 \tMSE_loss: 8.1457 \tTime: 0.0441\n",
      "Validation \tLoss_D: 0.9973\tLoss_G: 1.2729 \tMSE_loss: 4.2051 \tBest_loss: 4.2051\n",
      "[158/200][141/142]\tLoss_D: 1.0227\tLoss_G: 1.2182 \tMSE_loss: 3.5213 \tTime: 0.0451\n",
      "Validation \tLoss_D: 0.9987\tLoss_G: 1.2803 \tMSE_loss: 4.2941 \tBest_loss: 4.2051\n",
      "[159/200][141/142]\tLoss_D: 1.0980\tLoss_G: 1.0945 \tMSE_loss: 3.9013 \tTime: 0.0446\n",
      "Validation \tLoss_D: 1.0158\tLoss_G: 1.2455 \tMSE_loss: 4.2585 \tBest_loss: 4.2051\n",
      "[160/200][141/142]\tLoss_D: 1.0029\tLoss_G: 1.4245 \tMSE_loss: 4.8093 \tTime: 0.0470\n",
      "Validation \tLoss_D: 1.0083\tLoss_G: 1.2709 \tMSE_loss: 4.2626 \tBest_loss: 4.2051\n",
      "[161/200][141/142]\tLoss_D: 1.0679\tLoss_G: 1.4059 \tMSE_loss: 5.7842 \tTime: 0.0464\n",
      "Validation \tLoss_D: 1.0165\tLoss_G: 1.2671 \tMSE_loss: 4.2649 \tBest_loss: 4.2051\n",
      "[162/200][141/142]\tLoss_D: 0.9824\tLoss_G: 1.3564 \tMSE_loss: 4.4946 \tTime: 0.0452\n",
      "Validation \tLoss_D: 1.0194\tLoss_G: 1.2465 \tMSE_loss: 4.2850 \tBest_loss: 4.2051\n",
      "[163/200][141/142]\tLoss_D: 1.1726\tLoss_G: 1.1214 \tMSE_loss: 4.6710 \tTime: 0.0457\n",
      "Validation \tLoss_D: 1.0340\tLoss_G: 1.2235 \tMSE_loss: 4.2335 \tBest_loss: 4.2051\n",
      "[164/200][141/142]\tLoss_D: 1.0689\tLoss_G: 1.5633 \tMSE_loss: 6.4090 \tTime: 0.0394\n",
      "Validation \tLoss_D: 1.0295\tLoss_G: 1.2456 \tMSE_loss: 4.2378 \tBest_loss: 4.2051\n",
      "[165/200][141/142]\tLoss_D: 1.0263\tLoss_G: 1.3245 \tMSE_loss: 5.4755 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.0472\tLoss_G: 1.1983 \tMSE_loss: 4.1866 \tBest_loss: 4.1866\n",
      "[166/200][141/142]\tLoss_D: 0.9885\tLoss_G: 1.5707 \tMSE_loss: 5.7589 \tTime: 0.0433\n",
      "Validation \tLoss_D: 1.0391\tLoss_G: 1.2201 \tMSE_loss: 4.2458 \tBest_loss: 4.1866\n",
      "[167/200][141/142]\tLoss_D: 1.4266\tLoss_G: 1.2624 \tMSE_loss: 4.5169 \tTime: 0.0499\n",
      "Validation \tLoss_D: 1.0457\tLoss_G: 1.1930 \tMSE_loss: 4.2316 \tBest_loss: 4.1866\n",
      "[168/200][141/142]\tLoss_D: 1.1468\tLoss_G: 1.2258 \tMSE_loss: 2.6621 \tTime: 0.0473\n",
      "Validation \tLoss_D: 1.0519\tLoss_G: 1.1783 \tMSE_loss: 4.1850 \tBest_loss: 4.1850\n",
      "[169/200][141/142]\tLoss_D: 0.9747\tLoss_G: 1.2488 \tMSE_loss: 5.0079 \tTime: 0.0467\n",
      "Validation \tLoss_D: 1.0656\tLoss_G: 1.1658 \tMSE_loss: 4.1754 \tBest_loss: 4.1754\n",
      "[170/200][141/142]\tLoss_D: 0.9812\tLoss_G: 1.1878 \tMSE_loss: 7.1087 \tTime: 0.0517\n",
      "Validation \tLoss_D: 1.0742\tLoss_G: 1.1585 \tMSE_loss: 4.1540 \tBest_loss: 4.1540\n",
      "[171/200][141/142]\tLoss_D: 0.9653\tLoss_G: 1.2773 \tMSE_loss: 5.9962 \tTime: 0.0523\n",
      "Validation \tLoss_D: 1.0655\tLoss_G: 1.1881 \tMSE_loss: 4.2097 \tBest_loss: 4.1540\n",
      "[172/200][141/142]\tLoss_D: 1.2566\tLoss_G: 1.4864 \tMSE_loss: 3.9577 \tTime: 0.0481\n",
      "Validation \tLoss_D: 1.0763\tLoss_G: 1.1531 \tMSE_loss: 4.1354 \tBest_loss: 4.1354\n",
      "[173/200][141/142]\tLoss_D: 1.0496\tLoss_G: 1.3488 \tMSE_loss: 4.5469 \tTime: 0.0473\n",
      "Validation \tLoss_D: 1.0804\tLoss_G: 1.1407 \tMSE_loss: 4.0641 \tBest_loss: 4.0641\n",
      "[174/200][141/142]\tLoss_D: 0.9098\tLoss_G: 1.3746 \tMSE_loss: 4.9654 \tTime: 0.0490\n",
      "Validation \tLoss_D: 1.0796\tLoss_G: 1.1561 \tMSE_loss: 4.2258 \tBest_loss: 4.0641\n",
      "[175/200][141/142]\tLoss_D: 1.2166\tLoss_G: 1.4796 \tMSE_loss: 3.1457 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.0995\tLoss_G: 1.1198 \tMSE_loss: 4.0842 \tBest_loss: 4.0641\n",
      "[176/200][141/142]\tLoss_D: 1.0765\tLoss_G: 1.2800 \tMSE_loss: 7.5393 \tTime: 0.0489\n",
      "Validation \tLoss_D: 1.1075\tLoss_G: 1.1117 \tMSE_loss: 4.0989 \tBest_loss: 4.0641\n",
      "[177/200][141/142]\tLoss_D: 1.0808\tLoss_G: 1.2823 \tMSE_loss: 4.5414 \tTime: 0.0464\n",
      "Validation \tLoss_D: 1.1059\tLoss_G: 1.1208 \tMSE_loss: 4.1101 \tBest_loss: 4.0641\n",
      "[178/200][141/142]\tLoss_D: 1.1515\tLoss_G: 1.2007 \tMSE_loss: 5.6338 \tTime: 0.0479\n",
      "Validation \tLoss_D: 1.1097\tLoss_G: 1.1036 \tMSE_loss: 4.0667 \tBest_loss: 4.0641\n",
      "[179/200][141/142]\tLoss_D: 0.7997\tLoss_G: 1.5193 \tMSE_loss: 5.0574 \tTime: 0.0494\n",
      "Validation \tLoss_D: 1.1034\tLoss_G: 1.1284 \tMSE_loss: 4.1479 \tBest_loss: 4.0641\n",
      "[180/200][141/142]\tLoss_D: 1.0949\tLoss_G: 1.1959 \tMSE_loss: 6.1540 \tTime: 0.0508\n",
      "Validation \tLoss_D: 1.1184\tLoss_G: 1.0859 \tMSE_loss: 4.1018 \tBest_loss: 4.0641\n",
      "[181/200][141/142]\tLoss_D: 1.1862\tLoss_G: 1.3721 \tMSE_loss: 4.3021 \tTime: 0.0497\n",
      "Validation \tLoss_D: 1.1214\tLoss_G: 1.0904 \tMSE_loss: 4.1064 \tBest_loss: 4.0641\n",
      "[182/200][141/142]\tLoss_D: 1.1854\tLoss_G: 1.1951 \tMSE_loss: 3.8651 \tTime: 0.0485\n",
      "Validation \tLoss_D: 1.1235\tLoss_G: 1.0871 \tMSE_loss: 4.0208 \tBest_loss: 4.0208\n",
      "[183/200][141/142]\tLoss_D: 1.1973\tLoss_G: 1.3531 \tMSE_loss: 6.6905 \tTime: 0.0475\n",
      "Validation \tLoss_D: 1.1220\tLoss_G: 1.1042 \tMSE_loss: 4.0818 \tBest_loss: 4.0208\n",
      "[184/200][141/142]\tLoss_D: 1.0015\tLoss_G: 1.3072 \tMSE_loss: 4.2839 \tTime: 0.0484\n",
      "Validation \tLoss_D: 1.1409\tLoss_G: 1.0660 \tMSE_loss: 4.0316 \tBest_loss: 4.0208\n",
      "[185/200][141/142]\tLoss_D: 0.9514\tLoss_G: 1.2892 \tMSE_loss: 4.6771 \tTime: 0.0424\n",
      "Validation \tLoss_D: 1.1450\tLoss_G: 1.0642 \tMSE_loss: 4.0628 \tBest_loss: 4.0208\n",
      "[186/200][141/142]\tLoss_D: 1.0139\tLoss_G: 1.0253 \tMSE_loss: 4.8356 \tTime: 0.0470\n",
      "Validation \tLoss_D: 1.1520\tLoss_G: 1.0559 \tMSE_loss: 4.0426 \tBest_loss: 4.0208\n",
      "[187/200][141/142]\tLoss_D: 1.2696\tLoss_G: 1.0028 \tMSE_loss: 4.7245 \tTime: 0.0456\n",
      "Validation \tLoss_D: 1.1658\tLoss_G: 1.0339 \tMSE_loss: 3.9733 \tBest_loss: 3.9733\n",
      "[188/200][141/142]\tLoss_D: 1.0884\tLoss_G: 1.3556 \tMSE_loss: 4.5126 \tTime: 0.0511\n",
      "Validation \tLoss_D: 1.1589\tLoss_G: 1.0471 \tMSE_loss: 4.0054 \tBest_loss: 3.9733\n",
      "[189/200][141/142]\tLoss_D: 1.0328\tLoss_G: 1.2196 \tMSE_loss: 4.1280 \tTime: 0.0456\n",
      "Validation \tLoss_D: 1.1615\tLoss_G: 1.0598 \tMSE_loss: 4.0631 \tBest_loss: 3.9733\n",
      "[190/200][141/142]\tLoss_D: 1.1698\tLoss_G: 1.1563 \tMSE_loss: 7.6250 \tTime: 0.0407\n",
      "Validation \tLoss_D: 1.1634\tLoss_G: 1.0591 \tMSE_loss: 4.0511 \tBest_loss: 3.9733\n",
      "[191/200][141/142]\tLoss_D: 1.2402\tLoss_G: 1.2046 \tMSE_loss: 5.0013 \tTime: 0.0529\n",
      "Validation \tLoss_D: 1.1697\tLoss_G: 1.0506 \tMSE_loss: 4.0040 \tBest_loss: 3.9733\n",
      "[192/200][141/142]\tLoss_D: 1.1305\tLoss_G: 1.0428 \tMSE_loss: 6.6161 \tTime: 0.0498\n",
      "Validation \tLoss_D: 1.1728\tLoss_G: 1.0370 \tMSE_loss: 4.0401 \tBest_loss: 3.9733\n",
      "[193/200][141/142]\tLoss_D: 0.9985\tLoss_G: 1.1701 \tMSE_loss: 6.2008 \tTime: 0.0488\n",
      "Validation \tLoss_D: 1.1777\tLoss_G: 1.0274 \tMSE_loss: 4.0076 \tBest_loss: 3.9733\n",
      "[194/200][141/142]\tLoss_D: 1.2466\tLoss_G: 1.0525 \tMSE_loss: 6.8401 \tTime: 0.0507\n",
      "Validation \tLoss_D: 1.1956\tLoss_G: 0.9945 \tMSE_loss: 3.9127 \tBest_loss: 3.9127\n",
      "[195/200][141/142]\tLoss_D: 1.0325\tLoss_G: 1.2167 \tMSE_loss: 5.0041 \tTime: 0.0469\n",
      "Validation \tLoss_D: 1.1896\tLoss_G: 1.0159 \tMSE_loss: 3.9945 \tBest_loss: 3.9127\n",
      "[196/200][141/142]\tLoss_D: 1.0751\tLoss_G: 1.3144 \tMSE_loss: 4.6310 \tTime: 0.0535\n",
      "Validation \tLoss_D: 1.2005\tLoss_G: 0.9965 \tMSE_loss: 3.9479 \tBest_loss: 3.9127\n",
      "[197/200][141/142]\tLoss_D: 1.1429\tLoss_G: 0.9390 \tMSE_loss: 8.2237 \tTime: 0.0473\n",
      "Validation \tLoss_D: 1.1992\tLoss_G: 1.0044 \tMSE_loss: 3.9312 \tBest_loss: 3.9127\n",
      "[198/200][141/142]\tLoss_D: 1.1295\tLoss_G: 1.1746 \tMSE_loss: 4.3830 \tTime: 0.0463\n",
      "Validation \tLoss_D: 1.2100\tLoss_G: 0.9911 \tMSE_loss: 3.9447 \tBest_loss: 3.9127\n",
      "[199/200][141/142]\tLoss_D: 1.0853\tLoss_G: 1.0576 \tMSE_loss: 4.4104 \tTime: 0.0455\n",
      "Validation \tLoss_D: 1.2151\tLoss_G: 0.9906 \tMSE_loss: 3.9082 \tBest_loss: 3.9082\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "torch.manual_seed(8)\n",
    "\n",
    "def train_GAN(G, D, optim_G, optim_D, loss_f, train_loader, test_loader, num_epochs, device):\n",
    "    test_size = len(test_loader)\n",
    "    best = np.inf \n",
    "    for epoch in range(num_epochs):\n",
    "        for i,data in enumerate(train_loader):\n",
    "            if i<140:\n",
    "                generator.train()\n",
    "                discriminator.train()   \n",
    "                starting_time = time.time()\n",
    "                sequence , target = data\n",
    "                sequence = sequence.to(device)\n",
    "                target = target.to(device)\n",
    "                generator_fake = G(sequence)\n",
    "                true_seq = torch.concat((sequence,target.reshape(-1,1,7)),dim=1)\n",
    "                fake_seq = torch.concat((sequence,generator_fake),dim=1)\n",
    "                # ========================\n",
    "                #   Train Discriminator\n",
    "                # ========================\n",
    "                # train with real data\n",
    "                \n",
    "                prediction = D(true_seq)\n",
    "\n",
    "                # train with fake data\n",
    "                \n",
    "                fake_predection = D(fake_seq)\n",
    "                d_loss = descriminator_loss(prediction, fake_predection)\n",
    "                # update D\n",
    "                \n",
    "                D.zero_grad()\n",
    "                d_loss.backward()\n",
    "                optim_D.step()\n",
    "\n",
    "                # ========================\n",
    "                #   Train Generator\n",
    "                # ========================\n",
    "                # train with fake data  \n",
    "                        \n",
    "                sequence , target = data\n",
    "                sequence = sequence.to(device)\n",
    "                target = target.to(device)\n",
    "                generator_fake = G(sequence)\n",
    "                fake_seq = torch.concat((sequence,generator_fake),dim=1)\n",
    "                fake_predection = D(fake_seq)\n",
    "                g_loss, mse_loss = generator_loss(generator_fake,target,fake_predection)\n",
    "                # update G\n",
    "                G.zero_grad()\n",
    "                g_loss.backward()\n",
    "                optim_G.step()   \n",
    "        print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f \\tMSE_loss: %.4f \\tTime: %.4f'% (epoch, num_epochs, i, len(train_loader), d_loss.item(), g_loss.item(),mse_loss.item(),time.time()-starting_time))\n",
    "        #evaluate \n",
    "        dis_loss = 0\n",
    "        gen_loss= 0\n",
    "        mse_loss = 0\n",
    "        diss_losses = []\n",
    "        gen_losses = []\n",
    "        mse_losses = []\n",
    "\n",
    "        for i,data in enumerate(test_loader):\n",
    "            generator.eval()\n",
    "            discriminator.eval()\n",
    "            with torch.no_grad():\n",
    "                sequence , target = data\n",
    "                sequence = sequence.to(device)\n",
    "                target = target.to(device)\n",
    "                generator_fake = G(sequence)\n",
    "                fake_seq = torch.concat((sequence,generator_fake),dim=1)\n",
    "                true_seq = torch.concat((sequence,target.reshape(-1,1,7)),dim=1)\n",
    "                fake_predection = D(fake_seq)\n",
    "                g_loss, mse = generator_loss(generator_fake,target,fake_predection)\n",
    "                prediction = D(true_seq)\n",
    "                d_loss = descriminator_loss(prediction, fake_predection)\n",
    "                dis_loss += d_loss.item()/test_size\n",
    "                gen_loss += g_loss.item()/test_size\n",
    "                mse_loss += mse.item()/test_size\n",
    "        diss_losses.append(dis_loss)\n",
    "        gen_losses.append(gen_loss)\n",
    "        mse_losses.append(mse_loss)\n",
    "\n",
    "        if mse_loss < best:\n",
    "            best = mse_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': G.state_dict(),\n",
    "                'optimizer_state_dict': optim_G.state_dict(),\n",
    "            }, 'lstm_gan.pth') \n",
    "        print('Validation \\tLoss_D: %.4f\\tLoss_G: %.4f \\tMSE_loss: %.4f \\tBest_loss: %.4f'% (dis_loss, gen_loss,mse_loss,best))\n",
    "    return diss_losses,gen_losses,mse_losses\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "epochs = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = GeneratorModel(n_sequence, n_features).to(torch.float64)\n",
    "discriminator = DiscriminatorModel(n_sequence,n_features).to(torch.float64)\n",
    "generator , discriminator = generator.to(device), discriminator.to(device)\n",
    "learning_rate1 = 0.00003\n",
    "learning_rate2 = 0.00003\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate1, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate2, betas=(0.5, 0.999))\n",
    "checkpoint_path = \"./train_checkpoints\"\n",
    "\n",
    "results = train_GAN(generator, discriminator, generator_optimizer, discriminator_optimizer, loss_fn, data_gen_train, data_gen_test, epochs, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "432c454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][141/142]\tLoss_D: 1.0274\tLoss_G: 1.1362 \tMSE_loss: 5.2902 \tTime: 0.0493\n",
      "Validation \tLoss_D: 1.2099\tLoss_G: 0.9836 \tMSE_loss: 3.9702 \tBest_loss: 3.9702\n",
      "[1/200][141/142]\tLoss_D: 1.2259\tLoss_G: 1.0222 \tMSE_loss: 3.8514 \tTime: 0.0372\n",
      "Validation \tLoss_D: 1.2149\tLoss_G: 0.9821 \tMSE_loss: 3.9236 \tBest_loss: 3.9236\n",
      "[2/200][141/142]\tLoss_D: 1.1886\tLoss_G: 1.1685 \tMSE_loss: 2.3082 \tTime: 0.0510\n",
      "Validation \tLoss_D: 1.2160\tLoss_G: 0.9816 \tMSE_loss: 3.9339 \tBest_loss: 3.9236\n",
      "[3/200][141/142]\tLoss_D: 1.0799\tLoss_G: 0.8178 \tMSE_loss: 4.4767 \tTime: 0.0476\n",
      "Validation \tLoss_D: 1.2212\tLoss_G: 0.9714 \tMSE_loss: 3.9235 \tBest_loss: 3.9235\n",
      "[4/200][141/142]\tLoss_D: 1.2099\tLoss_G: 1.3141 \tMSE_loss: 4.1960 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.2245\tLoss_G: 0.9700 \tMSE_loss: 3.9034 \tBest_loss: 3.9034\n",
      "[5/200][141/142]\tLoss_D: 1.2291\tLoss_G: 1.3092 \tMSE_loss: 4.6321 \tTime: 0.0488\n",
      "Validation \tLoss_D: 1.2228\tLoss_G: 0.9698 \tMSE_loss: 3.8883 \tBest_loss: 3.8883\n",
      "[6/200][141/142]\tLoss_D: 1.2782\tLoss_G: 1.1091 \tMSE_loss: 4.6643 \tTime: 0.0458\n",
      "Validation \tLoss_D: 1.2214\tLoss_G: 0.9674 \tMSE_loss: 3.9100 \tBest_loss: 3.8883\n",
      "[7/200][141/142]\tLoss_D: 1.2943\tLoss_G: 1.1075 \tMSE_loss: 5.2746 \tTime: 0.0431\n",
      "Validation \tLoss_D: 1.2233\tLoss_G: 0.9705 \tMSE_loss: 3.9824 \tBest_loss: 3.8883\n",
      "[8/200][141/142]\tLoss_D: 1.1613\tLoss_G: 0.9945 \tMSE_loss: 5.1287 \tTime: 0.0445\n",
      "Validation \tLoss_D: 1.2427\tLoss_G: 0.9458 \tMSE_loss: 3.8639 \tBest_loss: 3.8639\n",
      "[9/200][141/142]\tLoss_D: 1.1935\tLoss_G: 1.2012 \tMSE_loss: 5.3016 \tTime: 0.0382\n",
      "Validation \tLoss_D: 1.2337\tLoss_G: 0.9597 \tMSE_loss: 3.9489 \tBest_loss: 3.8639\n",
      "[10/200][141/142]\tLoss_D: 1.1056\tLoss_G: 0.9915 \tMSE_loss: 4.0414 \tTime: 0.0473\n",
      "Validation \tLoss_D: 1.2432\tLoss_G: 0.9456 \tMSE_loss: 3.8814 \tBest_loss: 3.8639\n",
      "[11/200][141/142]\tLoss_D: 1.0975\tLoss_G: 1.3840 \tMSE_loss: 5.5902 \tTime: 0.0457\n",
      "Validation \tLoss_D: 1.2346\tLoss_G: 0.9636 \tMSE_loss: 3.9337 \tBest_loss: 3.8639\n",
      "[12/200][141/142]\tLoss_D: 1.1328\tLoss_G: 1.0867 \tMSE_loss: 3.8727 \tTime: 0.0509\n",
      "Validation \tLoss_D: 1.2424\tLoss_G: 0.9519 \tMSE_loss: 3.9032 \tBest_loss: 3.8639\n",
      "[13/200][141/142]\tLoss_D: 1.1701\tLoss_G: 1.1111 \tMSE_loss: 4.1937 \tTime: 0.0484\n",
      "Validation \tLoss_D: 1.2461\tLoss_G: 0.9395 \tMSE_loss: 3.8580 \tBest_loss: 3.8580\n",
      "[14/200][141/142]\tLoss_D: 1.1850\tLoss_G: 0.9630 \tMSE_loss: 4.9555 \tTime: 0.0439\n",
      "Validation \tLoss_D: 1.2491\tLoss_G: 0.9383 \tMSE_loss: 3.9186 \tBest_loss: 3.8580\n",
      "[15/200][141/142]\tLoss_D: 1.0523\tLoss_G: 1.0337 \tMSE_loss: 7.8475 \tTime: 0.0442\n",
      "Validation \tLoss_D: 1.2532\tLoss_G: 0.9327 \tMSE_loss: 3.9063 \tBest_loss: 3.8580\n",
      "[16/200][141/142]\tLoss_D: 1.2502\tLoss_G: 1.1009 \tMSE_loss: 4.3912 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.2590\tLoss_G: 0.9307 \tMSE_loss: 3.9520 \tBest_loss: 3.8580\n",
      "[17/200][141/142]\tLoss_D: 1.3157\tLoss_G: 1.0065 \tMSE_loss: 5.0309 \tTime: 0.0431\n",
      "Validation \tLoss_D: 1.2509\tLoss_G: 0.9380 \tMSE_loss: 3.8670 \tBest_loss: 3.8580\n",
      "[18/200][141/142]\tLoss_D: 1.2298\tLoss_G: 1.0549 \tMSE_loss: 3.3925 \tTime: 0.0489\n",
      "Validation \tLoss_D: 1.2521\tLoss_G: 0.9317 \tMSE_loss: 3.8897 \tBest_loss: 3.8580\n",
      "[19/200][141/142]\tLoss_D: 1.1389\tLoss_G: 1.4249 \tMSE_loss: 3.8394 \tTime: 0.0456\n",
      "Validation \tLoss_D: 1.2488\tLoss_G: 0.9354 \tMSE_loss: 3.9076 \tBest_loss: 3.8580\n",
      "[20/200][141/142]\tLoss_D: 1.3671\tLoss_G: 0.9157 \tMSE_loss: 4.7407 \tTime: 0.0431\n",
      "Validation \tLoss_D: 1.2573\tLoss_G: 0.9232 \tMSE_loss: 3.8666 \tBest_loss: 3.8580\n",
      "[21/200][141/142]\tLoss_D: 0.9854\tLoss_G: 1.2219 \tMSE_loss: 5.9991 \tTime: 0.0405\n",
      "Validation \tLoss_D: 1.2563\tLoss_G: 0.9328 \tMSE_loss: 3.8846 \tBest_loss: 3.8580\n",
      "[22/200][141/142]\tLoss_D: 1.1576\tLoss_G: 1.2822 \tMSE_loss: 11.0361 \tTime: 0.0377\n",
      "Validation \tLoss_D: 1.2605\tLoss_G: 0.9232 \tMSE_loss: 3.8714 \tBest_loss: 3.8580\n",
      "[23/200][141/142]\tLoss_D: 1.0007\tLoss_G: 1.2007 \tMSE_loss: 4.8937 \tTime: 0.0402\n",
      "Validation \tLoss_D: 1.2604\tLoss_G: 0.9274 \tMSE_loss: 3.9143 \tBest_loss: 3.8580\n",
      "[24/200][141/142]\tLoss_D: 1.2047\tLoss_G: 0.9639 \tMSE_loss: 4.3601 \tTime: 0.0465\n",
      "Validation \tLoss_D: 1.2656\tLoss_G: 0.9141 \tMSE_loss: 3.8976 \tBest_loss: 3.8580\n",
      "[25/200][141/142]\tLoss_D: 1.2206\tLoss_G: 0.9809 \tMSE_loss: 4.6890 \tTime: 0.0503\n",
      "Validation \tLoss_D: 1.2676\tLoss_G: 0.9173 \tMSE_loss: 3.8915 \tBest_loss: 3.8580\n",
      "[26/200][141/142]\tLoss_D: 1.2305\tLoss_G: 1.2355 \tMSE_loss: 6.6937 \tTime: 0.0528\n",
      "Validation \tLoss_D: 1.2594\tLoss_G: 0.9285 \tMSE_loss: 3.9196 \tBest_loss: 3.8580\n",
      "[27/200][141/142]\tLoss_D: 1.2064\tLoss_G: 0.9793 \tMSE_loss: 6.0666 \tTime: 0.0490\n",
      "Validation \tLoss_D: 1.2658\tLoss_G: 0.9154 \tMSE_loss: 3.8458 \tBest_loss: 3.8458\n",
      "[28/200][141/142]\tLoss_D: 1.2092\tLoss_G: 1.2806 \tMSE_loss: 8.0751 \tTime: 0.0533\n",
      "Validation \tLoss_D: 1.2638\tLoss_G: 0.9163 \tMSE_loss: 3.9162 \tBest_loss: 3.8458\n",
      "[29/200][141/142]\tLoss_D: 0.9010\tLoss_G: 1.2139 \tMSE_loss: 11.2866 \tTime: 0.0471\n",
      "Validation \tLoss_D: 1.2734\tLoss_G: 0.9033 \tMSE_loss: 3.8342 \tBest_loss: 3.8342\n",
      "[30/200][141/142]\tLoss_D: 1.0998\tLoss_G: 0.9266 \tMSE_loss: 5.1481 \tTime: 0.0493\n",
      "Validation \tLoss_D: 1.2646\tLoss_G: 0.9149 \tMSE_loss: 3.8554 \tBest_loss: 3.8342\n",
      "[31/200][141/142]\tLoss_D: 1.1663\tLoss_G: 1.2792 \tMSE_loss: 5.1972 \tTime: 0.0541\n",
      "Validation \tLoss_D: 1.2665\tLoss_G: 0.9110 \tMSE_loss: 3.8662 \tBest_loss: 3.8342\n",
      "[32/200][141/142]\tLoss_D: 1.1203\tLoss_G: 1.2363 \tMSE_loss: 3.9715 \tTime: 0.0485\n",
      "Validation \tLoss_D: 1.2707\tLoss_G: 0.9070 \tMSE_loss: 3.8691 \tBest_loss: 3.8342\n",
      "[33/200][141/142]\tLoss_D: 1.4296\tLoss_G: 1.0250 \tMSE_loss: 4.9297 \tTime: 0.0464\n",
      "Validation \tLoss_D: 1.2645\tLoss_G: 0.9117 \tMSE_loss: 3.8207 \tBest_loss: 3.8207\n",
      "[34/200][141/142]\tLoss_D: 1.1395\tLoss_G: 1.1903 \tMSE_loss: 7.0118 \tTime: 0.0460\n",
      "Validation \tLoss_D: 1.2464\tLoss_G: 0.9414 \tMSE_loss: 3.9980 \tBest_loss: 3.8207\n",
      "[35/200][141/142]\tLoss_D: 1.1966\tLoss_G: 1.1188 \tMSE_loss: 5.2556 \tTime: 0.0459\n",
      "Validation \tLoss_D: 1.2494\tLoss_G: 0.9435 \tMSE_loss: 4.0117 \tBest_loss: 3.8207\n",
      "[36/200][141/142]\tLoss_D: 1.0875\tLoss_G: 1.0964 \tMSE_loss: 5.4051 \tTime: 0.0470\n",
      "Validation \tLoss_D: 1.2676\tLoss_G: 0.9091 \tMSE_loss: 3.8423 \tBest_loss: 3.8207\n",
      "[37/200][141/142]\tLoss_D: 1.1630\tLoss_G: 0.9757 \tMSE_loss: 4.5805 \tTime: 0.0502\n",
      "Validation \tLoss_D: 1.2716\tLoss_G: 0.9027 \tMSE_loss: 3.8666 \tBest_loss: 3.8207\n",
      "[38/200][141/142]\tLoss_D: 1.1402\tLoss_G: 1.1270 \tMSE_loss: 4.8638 \tTime: 0.0511\n",
      "Validation \tLoss_D: 1.2716\tLoss_G: 0.8943 \tMSE_loss: 3.8753 \tBest_loss: 3.8207\n",
      "[39/200][141/142]\tLoss_D: 1.2591\tLoss_G: 1.0632 \tMSE_loss: 5.6970 \tTime: 0.0472\n",
      "Validation \tLoss_D: 1.2742\tLoss_G: 0.9056 \tMSE_loss: 3.9140 \tBest_loss: 3.8207\n",
      "[40/200][141/142]\tLoss_D: 1.0387\tLoss_G: 1.1863 \tMSE_loss: 7.8099 \tTime: 0.0397\n",
      "Validation \tLoss_D: 1.2832\tLoss_G: 0.8869 \tMSE_loss: 3.8943 \tBest_loss: 3.8207\n",
      "[41/200][141/142]\tLoss_D: 1.1658\tLoss_G: 1.2584 \tMSE_loss: 4.9201 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.2792\tLoss_G: 0.8939 \tMSE_loss: 3.9013 \tBest_loss: 3.8207\n",
      "[42/200][141/142]\tLoss_D: 1.0690\tLoss_G: 0.9669 \tMSE_loss: 7.3304 \tTime: 0.0554\n",
      "Validation \tLoss_D: 1.2818\tLoss_G: 0.8914 \tMSE_loss: 3.8533 \tBest_loss: 3.8207\n",
      "[43/200][141/142]\tLoss_D: 0.9662\tLoss_G: 1.3768 \tMSE_loss: 5.8453 \tTime: 0.0402\n",
      "Validation \tLoss_D: 1.2738\tLoss_G: 0.9074 \tMSE_loss: 3.9895 \tBest_loss: 3.8207\n",
      "[44/200][141/142]\tLoss_D: 1.2086\tLoss_G: 1.3037 \tMSE_loss: 6.2772 \tTime: 0.0390\n",
      "Validation \tLoss_D: 1.2832\tLoss_G: 0.8832 \tMSE_loss: 3.8963 \tBest_loss: 3.8207\n",
      "[45/200][141/142]\tLoss_D: 1.1875\tLoss_G: 1.1963 \tMSE_loss: 4.9051 \tTime: 0.0504\n",
      "Validation \tLoss_D: 1.2776\tLoss_G: 0.8986 \tMSE_loss: 3.8962 \tBest_loss: 3.8207\n",
      "[46/200][141/142]\tLoss_D: 1.0852\tLoss_G: 1.3182 \tMSE_loss: 5.0877 \tTime: 0.0474\n",
      "Validation \tLoss_D: 1.2755\tLoss_G: 0.8976 \tMSE_loss: 3.9304 \tBest_loss: 3.8207\n",
      "[47/200][141/142]\tLoss_D: 1.2694\tLoss_G: 0.9575 \tMSE_loss: 4.6416 \tTime: 0.0442\n",
      "Validation \tLoss_D: 1.2770\tLoss_G: 0.8933 \tMSE_loss: 3.8242 \tBest_loss: 3.8207\n",
      "[48/200][141/142]\tLoss_D: 1.2099\tLoss_G: 1.1356 \tMSE_loss: 2.9862 \tTime: 0.0463\n",
      "Validation \tLoss_D: 1.2806\tLoss_G: 0.8958 \tMSE_loss: 3.8382 \tBest_loss: 3.8207\n",
      "[49/200][141/142]\tLoss_D: 1.0903\tLoss_G: 1.1832 \tMSE_loss: 6.7673 \tTime: 0.0441\n",
      "Validation \tLoss_D: 1.2744\tLoss_G: 0.9089 \tMSE_loss: 3.9341 \tBest_loss: 3.8207\n",
      "[50/200][141/142]\tLoss_D: 1.2503\tLoss_G: 0.8560 \tMSE_loss: 5.0895 \tTime: 0.0505\n",
      "Validation \tLoss_D: 1.2869\tLoss_G: 0.8822 \tMSE_loss: 3.9492 \tBest_loss: 3.8207\n",
      "[51/200][141/142]\tLoss_D: 1.1558\tLoss_G: 1.1381 \tMSE_loss: 4.8983 \tTime: 0.0500\n",
      "Validation \tLoss_D: 1.2884\tLoss_G: 0.8798 \tMSE_loss: 3.8596 \tBest_loss: 3.8207\n",
      "[52/200][141/142]\tLoss_D: 1.1675\tLoss_G: 1.2025 \tMSE_loss: 3.4729 \tTime: 0.0483\n",
      "Validation \tLoss_D: 1.2848\tLoss_G: 0.8869 \tMSE_loss: 3.8842 \tBest_loss: 3.8207\n",
      "[53/200][141/142]\tLoss_D: 1.1344\tLoss_G: 1.0443 \tMSE_loss: 3.8566 \tTime: 0.0447\n",
      "Validation \tLoss_D: 1.2882\tLoss_G: 0.8829 \tMSE_loss: 3.8936 \tBest_loss: 3.8207\n",
      "[54/200][141/142]\tLoss_D: 1.1550\tLoss_G: 1.1568 \tMSE_loss: 4.2540 \tTime: 0.0446\n",
      "Validation \tLoss_D: 1.2877\tLoss_G: 0.8733 \tMSE_loss: 3.8418 \tBest_loss: 3.8207\n",
      "[55/200][141/142]\tLoss_D: 1.2329\tLoss_G: 1.3204 \tMSE_loss: 5.1992 \tTime: 0.0413\n",
      "Validation \tLoss_D: 1.2865\tLoss_G: 0.8713 \tMSE_loss: 3.8471 \tBest_loss: 3.8207\n",
      "[56/200][141/142]\tLoss_D: 0.9936\tLoss_G: 1.2201 \tMSE_loss: 4.9619 \tTime: 0.0458\n",
      "Validation \tLoss_D: 1.2898\tLoss_G: 0.8656 \tMSE_loss: 3.8373 \tBest_loss: 3.8207\n",
      "[57/200][141/142]\tLoss_D: 1.3061\tLoss_G: 1.0258 \tMSE_loss: 2.9662 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.2934\tLoss_G: 0.8673 \tMSE_loss: 3.8423 \tBest_loss: 3.8207\n",
      "[58/200][141/142]\tLoss_D: 1.1979\tLoss_G: 1.0867 \tMSE_loss: 6.2115 \tTime: 0.0476\n",
      "Validation \tLoss_D: 1.2920\tLoss_G: 0.8769 \tMSE_loss: 3.9196 \tBest_loss: 3.8207\n",
      "[59/200][141/142]\tLoss_D: 1.0714\tLoss_G: 1.3262 \tMSE_loss: 5.5983 \tTime: 0.0485\n",
      "Validation \tLoss_D: 1.2901\tLoss_G: 0.8726 \tMSE_loss: 3.8881 \tBest_loss: 3.8207\n",
      "[60/200][141/142]\tLoss_D: 1.2860\tLoss_G: 0.9382 \tMSE_loss: 4.0941 \tTime: 0.0482\n",
      "Validation \tLoss_D: 1.2950\tLoss_G: 0.8667 \tMSE_loss: 3.8738 \tBest_loss: 3.8207\n",
      "[61/200][141/142]\tLoss_D: 1.2977\tLoss_G: 1.1185 \tMSE_loss: 4.6638 \tTime: 0.0431\n",
      "Validation \tLoss_D: 1.2879\tLoss_G: 0.8764 \tMSE_loss: 3.8692 \tBest_loss: 3.8207\n",
      "[62/200][141/142]\tLoss_D: 0.9880\tLoss_G: 1.1358 \tMSE_loss: 4.0190 \tTime: 0.0488\n",
      "Validation \tLoss_D: 1.2958\tLoss_G: 0.8683 \tMSE_loss: 3.8664 \tBest_loss: 3.8207\n",
      "[63/200][141/142]\tLoss_D: 1.1119\tLoss_G: 1.1123 \tMSE_loss: 5.1842 \tTime: 0.0533\n",
      "Validation \tLoss_D: 1.2916\tLoss_G: 0.8666 \tMSE_loss: 3.8576 \tBest_loss: 3.8207\n",
      "[64/200][141/142]\tLoss_D: 1.1078\tLoss_G: 1.0108 \tMSE_loss: 5.9109 \tTime: 0.0457\n",
      "Validation \tLoss_D: 1.2925\tLoss_G: 0.8697 \tMSE_loss: 3.8930 \tBest_loss: 3.8207\n",
      "[65/200][141/142]\tLoss_D: 1.1217\tLoss_G: 1.2478 \tMSE_loss: 5.1125 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.2981\tLoss_G: 0.8599 \tMSE_loss: 3.8243 \tBest_loss: 3.8207\n",
      "[66/200][141/142]\tLoss_D: 1.0941\tLoss_G: 0.9965 \tMSE_loss: 5.6532 \tTime: 0.0387\n",
      "Validation \tLoss_D: 1.2966\tLoss_G: 0.8665 \tMSE_loss: 3.8647 \tBest_loss: 3.8207\n",
      "[67/200][141/142]\tLoss_D: 1.2435\tLoss_G: 1.2230 \tMSE_loss: 3.7052 \tTime: 0.0406\n",
      "Validation \tLoss_D: 1.2911\tLoss_G: 0.8698 \tMSE_loss: 3.8405 \tBest_loss: 3.8207\n",
      "[68/200][141/142]\tLoss_D: 1.1584\tLoss_G: 1.0529 \tMSE_loss: 7.4374 \tTime: 0.0414\n",
      "Validation \tLoss_D: 1.2979\tLoss_G: 0.8646 \tMSE_loss: 3.8411 \tBest_loss: 3.8207\n",
      "[69/200][141/142]\tLoss_D: 1.3459\tLoss_G: 0.9066 \tMSE_loss: 3.7134 \tTime: 0.0481\n",
      "Validation \tLoss_D: 1.3054\tLoss_G: 0.8566 \tMSE_loss: 3.8839 \tBest_loss: 3.8207\n",
      "[70/200][141/142]\tLoss_D: 1.1353\tLoss_G: 1.0870 \tMSE_loss: 3.7811 \tTime: 0.0495\n",
      "Validation \tLoss_D: 1.2986\tLoss_G: 0.8690 \tMSE_loss: 3.8481 \tBest_loss: 3.8207\n",
      "[71/200][141/142]\tLoss_D: 1.0970\tLoss_G: 1.2253 \tMSE_loss: 8.6583 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.2995\tLoss_G: 0.8652 \tMSE_loss: 3.8439 \tBest_loss: 3.8207\n",
      "[72/200][141/142]\tLoss_D: 1.1949\tLoss_G: 1.1109 \tMSE_loss: 6.1412 \tTime: 0.0473\n",
      "Validation \tLoss_D: 1.2993\tLoss_G: 0.8646 \tMSE_loss: 3.8410 \tBest_loss: 3.8207\n",
      "[73/200][141/142]\tLoss_D: 1.3979\tLoss_G: 1.0179 \tMSE_loss: 4.2771 \tTime: 0.0509\n",
      "Validation \tLoss_D: 1.3014\tLoss_G: 0.8635 \tMSE_loss: 3.9111 \tBest_loss: 3.8207\n",
      "[74/200][141/142]\tLoss_D: 1.2151\tLoss_G: 1.0161 \tMSE_loss: 3.9718 \tTime: 0.0472\n",
      "Validation \tLoss_D: 1.3015\tLoss_G: 0.8680 \tMSE_loss: 3.8324 \tBest_loss: 3.8207\n",
      "[75/200][141/142]\tLoss_D: 1.0030\tLoss_G: 1.3016 \tMSE_loss: 5.6878 \tTime: 0.0421\n",
      "Validation \tLoss_D: 1.2962\tLoss_G: 0.8695 \tMSE_loss: 3.8080 \tBest_loss: 3.8080\n",
      "[76/200][141/142]\tLoss_D: 0.9986\tLoss_G: 1.2715 \tMSE_loss: 5.8901 \tTime: 0.0485\n",
      "Validation \tLoss_D: 1.3021\tLoss_G: 0.8622 \tMSE_loss: 3.8577 \tBest_loss: 3.8080\n",
      "[77/200][141/142]\tLoss_D: 1.2030\tLoss_G: 1.0774 \tMSE_loss: 5.4037 \tTime: 0.0484\n",
      "Validation \tLoss_D: 1.3022\tLoss_G: 0.8636 \tMSE_loss: 3.8124 \tBest_loss: 3.8080\n",
      "[78/200][141/142]\tLoss_D: 1.1552\tLoss_G: 0.9301 \tMSE_loss: 3.5287 \tTime: 0.0455\n",
      "Validation \tLoss_D: 1.3042\tLoss_G: 0.8589 \tMSE_loss: 3.8425 \tBest_loss: 3.8080\n",
      "[79/200][141/142]\tLoss_D: 1.1677\tLoss_G: 1.0214 \tMSE_loss: 3.8190 \tTime: 0.0446\n",
      "Validation \tLoss_D: 1.3000\tLoss_G: 0.8642 \tMSE_loss: 3.7828 \tBest_loss: 3.7828\n",
      "[80/200][141/142]\tLoss_D: 1.3373\tLoss_G: 1.1528 \tMSE_loss: 4.5282 \tTime: 0.0480\n",
      "Validation \tLoss_D: 1.3075\tLoss_G: 0.8615 \tMSE_loss: 3.8391 \tBest_loss: 3.7828\n",
      "[81/200][141/142]\tLoss_D: 1.0793\tLoss_G: 1.2136 \tMSE_loss: 4.6856 \tTime: 0.0402\n",
      "Validation \tLoss_D: 1.3017\tLoss_G: 0.8696 \tMSE_loss: 3.8506 \tBest_loss: 3.7828\n",
      "[82/200][141/142]\tLoss_D: 1.4119\tLoss_G: 0.9490 \tMSE_loss: 3.5915 \tTime: 0.0443\n",
      "Validation \tLoss_D: 1.3110\tLoss_G: 0.8488 \tMSE_loss: 3.8397 \tBest_loss: 3.7828\n",
      "[83/200][141/142]\tLoss_D: 1.3179\tLoss_G: 1.0246 \tMSE_loss: 3.1171 \tTime: 0.0436\n",
      "Validation \tLoss_D: 1.3023\tLoss_G: 0.8646 \tMSE_loss: 3.8092 \tBest_loss: 3.7828\n",
      "[84/200][141/142]\tLoss_D: 1.2570\tLoss_G: 0.7525 \tMSE_loss: 4.2227 \tTime: 0.0469\n",
      "Validation \tLoss_D: 1.3085\tLoss_G: 0.8670 \tMSE_loss: 3.9060 \tBest_loss: 3.7828\n",
      "[85/200][141/142]\tLoss_D: 1.1250\tLoss_G: 0.8971 \tMSE_loss: 4.0517 \tTime: 0.0465\n",
      "Validation \tLoss_D: 1.3051\tLoss_G: 0.8615 \tMSE_loss: 3.8701 \tBest_loss: 3.7828\n",
      "[86/200][141/142]\tLoss_D: 1.2747\tLoss_G: 1.0822 \tMSE_loss: 4.5737 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.3053\tLoss_G: 0.8578 \tMSE_loss: 3.8172 \tBest_loss: 3.7828\n",
      "[87/200][141/142]\tLoss_D: 1.0638\tLoss_G: 0.9098 \tMSE_loss: 4.4431 \tTime: 0.0455\n",
      "Validation \tLoss_D: 1.3056\tLoss_G: 0.8669 \tMSE_loss: 3.8035 \tBest_loss: 3.7828\n",
      "[88/200][141/142]\tLoss_D: 1.2013\tLoss_G: 1.0385 \tMSE_loss: 6.2094 \tTime: 0.0441\n",
      "Validation \tLoss_D: 1.3053\tLoss_G: 0.8630 \tMSE_loss: 3.8081 \tBest_loss: 3.7828\n",
      "[89/200][141/142]\tLoss_D: 1.0452\tLoss_G: 1.4569 \tMSE_loss: 4.4115 \tTime: 0.0423\n",
      "Validation \tLoss_D: 1.3038\tLoss_G: 0.8650 \tMSE_loss: 3.7981 \tBest_loss: 3.7828\n",
      "[90/200][141/142]\tLoss_D: 1.0259\tLoss_G: 1.0746 \tMSE_loss: 3.9717 \tTime: 0.0427\n",
      "Validation \tLoss_D: 1.3048\tLoss_G: 0.8617 \tMSE_loss: 3.8204 \tBest_loss: 3.7828\n",
      "[91/200][141/142]\tLoss_D: 1.1327\tLoss_G: 1.1536 \tMSE_loss: 4.5808 \tTime: 0.0425\n",
      "Validation \tLoss_D: 1.3058\tLoss_G: 0.8708 \tMSE_loss: 3.8035 \tBest_loss: 3.7828\n",
      "[92/200][141/142]\tLoss_D: 0.9595\tLoss_G: 1.3741 \tMSE_loss: 6.4395 \tTime: 0.0496\n",
      "Validation \tLoss_D: 1.2990\tLoss_G: 0.8825 \tMSE_loss: 3.8486 \tBest_loss: 3.7828\n",
      "[93/200][141/142]\tLoss_D: 1.1178\tLoss_G: 1.2971 \tMSE_loss: 3.4401 \tTime: 0.0455\n",
      "Validation \tLoss_D: 1.3108\tLoss_G: 0.8600 \tMSE_loss: 3.8528 \tBest_loss: 3.7828\n",
      "[94/200][141/142]\tLoss_D: 1.0238\tLoss_G: 1.1296 \tMSE_loss: 3.4914 \tTime: 0.0457\n",
      "Validation \tLoss_D: 1.3040\tLoss_G: 0.8651 \tMSE_loss: 3.8330 \tBest_loss: 3.7828\n",
      "[95/200][141/142]\tLoss_D: 1.2074\tLoss_G: 1.1323 \tMSE_loss: 6.5205 \tTime: 0.0495\n",
      "Validation \tLoss_D: 1.3092\tLoss_G: 0.8680 \tMSE_loss: 3.8009 \tBest_loss: 3.7828\n",
      "[96/200][141/142]\tLoss_D: 1.2203\tLoss_G: 1.1906 \tMSE_loss: 5.4358 \tTime: 0.0414\n",
      "Validation \tLoss_D: 1.3036\tLoss_G: 0.8790 \tMSE_loss: 3.8199 \tBest_loss: 3.7828\n",
      "[97/200][141/142]\tLoss_D: 1.0011\tLoss_G: 1.2129 \tMSE_loss: 5.6165 \tTime: 0.0511\n",
      "Validation \tLoss_D: 1.3021\tLoss_G: 0.8820 \tMSE_loss: 3.8333 \tBest_loss: 3.7828\n",
      "[98/200][141/142]\tLoss_D: 1.1100\tLoss_G: 1.0452 \tMSE_loss: 4.7028 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.2925\tLoss_G: 0.8970 \tMSE_loss: 3.8042 \tBest_loss: 3.7828\n",
      "[99/200][141/142]\tLoss_D: 1.3037\tLoss_G: 1.1855 \tMSE_loss: 4.6568 \tTime: 0.0516\n",
      "Validation \tLoss_D: 1.3080\tLoss_G: 0.8665 \tMSE_loss: 3.7716 \tBest_loss: 3.7716\n",
      "[100/200][141/142]\tLoss_D: 1.1862\tLoss_G: 1.4275 \tMSE_loss: 3.2505 \tTime: 0.0481\n",
      "Validation \tLoss_D: 1.3098\tLoss_G: 0.8660 \tMSE_loss: 3.7732 \tBest_loss: 3.7716\n",
      "[101/200][141/142]\tLoss_D: 1.1101\tLoss_G: 1.0380 \tMSE_loss: 4.0035 \tTime: 0.0465\n",
      "Validation \tLoss_D: 1.3016\tLoss_G: 0.8738 \tMSE_loss: 3.8336 \tBest_loss: 3.7716\n",
      "[102/200][141/142]\tLoss_D: 1.2054\tLoss_G: 0.9844 \tMSE_loss: 3.6871 \tTime: 0.0502\n",
      "Validation \tLoss_D: 1.3109\tLoss_G: 0.8817 \tMSE_loss: 3.7795 \tBest_loss: 3.7716\n",
      "[103/200][141/142]\tLoss_D: 1.0969\tLoss_G: 1.0294 \tMSE_loss: 3.8478 \tTime: 0.0501\n",
      "Validation \tLoss_D: 1.3033\tLoss_G: 0.8748 \tMSE_loss: 3.7705 \tBest_loss: 3.7705\n",
      "[104/200][141/142]\tLoss_D: 1.2710\tLoss_G: 1.0679 \tMSE_loss: 3.1351 \tTime: 0.0487\n",
      "Validation \tLoss_D: 1.3057\tLoss_G: 0.8714 \tMSE_loss: 3.7814 \tBest_loss: 3.7705\n",
      "[105/200][141/142]\tLoss_D: 1.2911\tLoss_G: 0.9327 \tMSE_loss: 3.8082 \tTime: 0.0446\n",
      "Validation \tLoss_D: 1.3098\tLoss_G: 0.8716 \tMSE_loss: 3.8001 \tBest_loss: 3.7705\n",
      "[106/200][141/142]\tLoss_D: 1.0487\tLoss_G: 1.2481 \tMSE_loss: 4.4036 \tTime: 0.0493\n",
      "Validation \tLoss_D: 1.2912\tLoss_G: 0.8920 \tMSE_loss: 3.8923 \tBest_loss: 3.7705\n",
      "[107/200][141/142]\tLoss_D: 1.1461\tLoss_G: 1.4199 \tMSE_loss: 6.0823 \tTime: 0.0460\n",
      "Validation \tLoss_D: 1.3083\tLoss_G: 0.8722 \tMSE_loss: 3.7769 \tBest_loss: 3.7705\n",
      "[108/200][141/142]\tLoss_D: 1.1026\tLoss_G: 1.0189 \tMSE_loss: 4.0166 \tTime: 0.0470\n",
      "Validation \tLoss_D: 1.2956\tLoss_G: 0.8867 \tMSE_loss: 3.7659 \tBest_loss: 3.7659\n",
      "[109/200][141/142]\tLoss_D: 0.9930\tLoss_G: 1.1960 \tMSE_loss: 5.8666 \tTime: 0.0493\n",
      "Validation \tLoss_D: 1.3175\tLoss_G: 0.8644 \tMSE_loss: 3.7534 \tBest_loss: 3.7534\n",
      "[110/200][141/142]\tLoss_D: 1.1221\tLoss_G: 1.1733 \tMSE_loss: 3.7952 \tTime: 0.0436\n",
      "Validation \tLoss_D: 1.3126\tLoss_G: 0.8671 \tMSE_loss: 3.7815 \tBest_loss: 3.7534\n",
      "[111/200][141/142]\tLoss_D: 1.0832\tLoss_G: 1.0675 \tMSE_loss: 5.2848 \tTime: 0.0441\n",
      "Validation \tLoss_D: 1.2992\tLoss_G: 0.9129 \tMSE_loss: 3.7821 \tBest_loss: 3.7534\n",
      "[112/200][141/142]\tLoss_D: 1.0398\tLoss_G: 1.3504 \tMSE_loss: 4.6772 \tTime: 0.0500\n",
      "Validation \tLoss_D: 1.3165\tLoss_G: 0.8795 \tMSE_loss: 3.7905 \tBest_loss: 3.7534\n",
      "[113/200][141/142]\tLoss_D: 1.1105\tLoss_G: 1.1040 \tMSE_loss: 4.5094 \tTime: 0.0472\n",
      "Validation \tLoss_D: 1.3146\tLoss_G: 0.8785 \tMSE_loss: 3.7774 \tBest_loss: 3.7534\n",
      "[114/200][141/142]\tLoss_D: 1.1576\tLoss_G: 1.1853 \tMSE_loss: 3.4059 \tTime: 0.0517\n",
      "Validation \tLoss_D: 1.3109\tLoss_G: 0.8909 \tMSE_loss: 3.7717 \tBest_loss: 3.7534\n",
      "[115/200][141/142]\tLoss_D: 1.2225\tLoss_G: 1.0821 \tMSE_loss: 4.7487 \tTime: 0.0501\n",
      "Validation \tLoss_D: 1.3176\tLoss_G: 0.8760 \tMSE_loss: 3.7573 \tBest_loss: 3.7534\n",
      "[116/200][141/142]\tLoss_D: 1.1426\tLoss_G: 1.1499 \tMSE_loss: 4.8164 \tTime: 0.0445\n",
      "Validation \tLoss_D: 1.3150\tLoss_G: 0.8779 \tMSE_loss: 3.7582 \tBest_loss: 3.7534\n",
      "[117/200][141/142]\tLoss_D: 1.4008\tLoss_G: 1.2135 \tMSE_loss: 3.4577 \tTime: 0.0517\n",
      "Validation \tLoss_D: 1.3208\tLoss_G: 0.8796 \tMSE_loss: 3.7451 \tBest_loss: 3.7451\n",
      "[118/200][141/142]\tLoss_D: 1.2718\tLoss_G: 0.9421 \tMSE_loss: 4.6268 \tTime: 0.0510\n",
      "Validation \tLoss_D: 1.3187\tLoss_G: 0.8782 \tMSE_loss: 3.8102 \tBest_loss: 3.7451\n",
      "[119/200][141/142]\tLoss_D: 1.0166\tLoss_G: 0.9297 \tMSE_loss: 4.7016 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.3222\tLoss_G: 0.8816 \tMSE_loss: 3.7762 \tBest_loss: 3.7451\n",
      "[120/200][141/142]\tLoss_D: 1.2077\tLoss_G: 1.0895 \tMSE_loss: 4.5724 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.3296\tLoss_G: 0.8652 \tMSE_loss: 3.7683 \tBest_loss: 3.7451\n",
      "[121/200][141/142]\tLoss_D: 1.0280\tLoss_G: 1.1306 \tMSE_loss: 4.8200 \tTime: 0.0490\n",
      "Validation \tLoss_D: 1.3192\tLoss_G: 0.8867 \tMSE_loss: 3.7468 \tBest_loss: 3.7451\n",
      "[122/200][141/142]\tLoss_D: 1.2519\tLoss_G: 1.4165 \tMSE_loss: 4.8268 \tTime: 0.0489\n",
      "Validation \tLoss_D: 1.3230\tLoss_G: 0.8861 \tMSE_loss: 3.7659 \tBest_loss: 3.7451\n",
      "[123/200][141/142]\tLoss_D: 1.3281\tLoss_G: 1.0593 \tMSE_loss: 3.8439 \tTime: 0.0474\n",
      "Validation \tLoss_D: 1.3204\tLoss_G: 0.8867 \tMSE_loss: 3.7811 \tBest_loss: 3.7451\n",
      "[124/200][141/142]\tLoss_D: 1.1121\tLoss_G: 1.1297 \tMSE_loss: 4.8663 \tTime: 0.0476\n",
      "Validation \tLoss_D: 1.3178\tLoss_G: 0.8931 \tMSE_loss: 3.7977 \tBest_loss: 3.7451\n",
      "[125/200][141/142]\tLoss_D: 1.4352\tLoss_G: 0.9976 \tMSE_loss: 5.6225 \tTime: 0.0441\n",
      "Validation \tLoss_D: 1.3231\tLoss_G: 0.8841 \tMSE_loss: 3.8016 \tBest_loss: 3.7451\n",
      "[126/200][141/142]\tLoss_D: 1.0868\tLoss_G: 0.9528 \tMSE_loss: 4.7594 \tTime: 0.0472\n",
      "Validation \tLoss_D: 1.3292\tLoss_G: 0.8779 \tMSE_loss: 3.7614 \tBest_loss: 3.7451\n",
      "[127/200][141/142]\tLoss_D: 1.3759\tLoss_G: 1.4351 \tMSE_loss: 4.4854 \tTime: 0.0467\n",
      "Validation \tLoss_D: 1.3307\tLoss_G: 0.8790 \tMSE_loss: 3.7341 \tBest_loss: 3.7341\n",
      "[128/200][141/142]\tLoss_D: 1.2264\tLoss_G: 0.9705 \tMSE_loss: 3.4110 \tTime: 0.0464\n",
      "Validation \tLoss_D: 1.3234\tLoss_G: 0.8893 \tMSE_loss: 3.7816 \tBest_loss: 3.7341\n",
      "[129/200][141/142]\tLoss_D: 0.9770\tLoss_G: 1.0374 \tMSE_loss: 4.0550 \tTime: 0.0427\n",
      "Validation \tLoss_D: 1.3287\tLoss_G: 0.8794 \tMSE_loss: 3.7581 \tBest_loss: 3.7341\n",
      "[130/200][141/142]\tLoss_D: 1.2671\tLoss_G: 1.2457 \tMSE_loss: 5.5709 \tTime: 0.0492\n",
      "Validation \tLoss_D: 1.3190\tLoss_G: 0.8987 \tMSE_loss: 3.8768 \tBest_loss: 3.7341\n",
      "[131/200][141/142]\tLoss_D: 1.1067\tLoss_G: 1.2254 \tMSE_loss: 7.2050 \tTime: 0.0495\n",
      "Validation \tLoss_D: 1.3188\tLoss_G: 0.8973 \tMSE_loss: 3.7618 \tBest_loss: 3.7341\n",
      "[132/200][141/142]\tLoss_D: 1.1883\tLoss_G: 0.8423 \tMSE_loss: 4.3550 \tTime: 0.0474\n",
      "Validation \tLoss_D: 1.3345\tLoss_G: 0.8849 \tMSE_loss: 3.7411 \tBest_loss: 3.7341\n",
      "[133/200][141/142]\tLoss_D: 1.1970\tLoss_G: 0.8552 \tMSE_loss: 3.8526 \tTime: 0.0453\n",
      "Validation \tLoss_D: 1.3342\tLoss_G: 0.8781 \tMSE_loss: 3.7469 \tBest_loss: 3.7341\n",
      "[134/200][141/142]\tLoss_D: 1.2380\tLoss_G: 0.9553 \tMSE_loss: 3.5601 \tTime: 0.0427\n",
      "Validation \tLoss_D: 1.3391\tLoss_G: 0.8697 \tMSE_loss: 3.7689 \tBest_loss: 3.7341\n",
      "[135/200][141/142]\tLoss_D: 1.3270\tLoss_G: 1.2579 \tMSE_loss: 3.8358 \tTime: 0.0452\n",
      "Validation \tLoss_D: 1.3368\tLoss_G: 0.8710 \tMSE_loss: 3.7595 \tBest_loss: 3.7341\n",
      "[136/200][141/142]\tLoss_D: 1.3161\tLoss_G: 1.0500 \tMSE_loss: 5.6405 \tTime: 0.0474\n",
      "Validation \tLoss_D: 1.3384\tLoss_G: 0.8857 \tMSE_loss: 3.7960 \tBest_loss: 3.7341\n",
      "[137/200][141/142]\tLoss_D: 1.1296\tLoss_G: 1.1487 \tMSE_loss: 6.3966 \tTime: 0.0457\n",
      "Validation \tLoss_D: 1.3428\tLoss_G: 0.8699 \tMSE_loss: 3.7916 \tBest_loss: 3.7341\n",
      "[138/200][141/142]\tLoss_D: 1.1579\tLoss_G: 1.1536 \tMSE_loss: 5.6880 \tTime: 0.0421\n",
      "Validation \tLoss_D: 1.3364\tLoss_G: 0.8740 \tMSE_loss: 3.7685 \tBest_loss: 3.7341\n",
      "[139/200][141/142]\tLoss_D: 1.1751\tLoss_G: 1.1137 \tMSE_loss: 3.6413 \tTime: 0.0471\n",
      "Validation \tLoss_D: 1.3362\tLoss_G: 0.8849 \tMSE_loss: 3.7901 \tBest_loss: 3.7341\n",
      "[140/200][141/142]\tLoss_D: 1.2865\tLoss_G: 1.0641 \tMSE_loss: 4.7389 \tTime: 0.0492\n",
      "Validation \tLoss_D: 1.3393\tLoss_G: 0.8789 \tMSE_loss: 3.7744 \tBest_loss: 3.7341\n",
      "[141/200][141/142]\tLoss_D: 1.2052\tLoss_G: 0.8473 \tMSE_loss: 3.7384 \tTime: 0.0394\n",
      "Validation \tLoss_D: 1.3452\tLoss_G: 0.8669 \tMSE_loss: 3.7349 \tBest_loss: 3.7341\n",
      "[142/200][141/142]\tLoss_D: 1.1485\tLoss_G: 1.0802 \tMSE_loss: 5.4793 \tTime: 0.0497\n",
      "Validation \tLoss_D: 1.3407\tLoss_G: 0.8695 \tMSE_loss: 3.7773 \tBest_loss: 3.7341\n",
      "[143/200][141/142]\tLoss_D: 1.3082\tLoss_G: 1.2397 \tMSE_loss: 3.3402 \tTime: 0.0433\n",
      "Validation \tLoss_D: 1.3553\tLoss_G: 0.8647 \tMSE_loss: 3.7856 \tBest_loss: 3.7341\n",
      "[144/200][141/142]\tLoss_D: 1.1247\tLoss_G: 1.0117 \tMSE_loss: 4.0895 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.3526\tLoss_G: 0.8648 \tMSE_loss: 3.7538 \tBest_loss: 3.7341\n",
      "[145/200][141/142]\tLoss_D: 1.3341\tLoss_G: 1.0307 \tMSE_loss: 3.2784 \tTime: 0.0456\n",
      "Validation \tLoss_D: 1.3459\tLoss_G: 0.8766 \tMSE_loss: 3.7763 \tBest_loss: 3.7341\n",
      "[146/200][141/142]\tLoss_D: 1.1213\tLoss_G: 1.0561 \tMSE_loss: 3.1751 \tTime: 0.0533\n",
      "Validation \tLoss_D: 1.3559\tLoss_G: 0.8612 \tMSE_loss: 3.7732 \tBest_loss: 3.7341\n",
      "[147/200][141/142]\tLoss_D: 1.1939\tLoss_G: 0.9458 \tMSE_loss: 4.3235 \tTime: 0.0538\n",
      "Validation \tLoss_D: 1.3522\tLoss_G: 0.8633 \tMSE_loss: 3.7583 \tBest_loss: 3.7341\n",
      "[148/200][141/142]\tLoss_D: 1.1464\tLoss_G: 1.1471 \tMSE_loss: 5.3654 \tTime: 0.0577\n",
      "Validation \tLoss_D: 1.3635\tLoss_G: 0.8512 \tMSE_loss: 3.7791 \tBest_loss: 3.7341\n",
      "[149/200][141/142]\tLoss_D: 1.1311\tLoss_G: 1.1140 \tMSE_loss: 3.9763 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.3520\tLoss_G: 0.8684 \tMSE_loss: 3.7602 \tBest_loss: 3.7341\n",
      "[150/200][141/142]\tLoss_D: 1.1223\tLoss_G: 1.1704 \tMSE_loss: 4.5074 \tTime: 0.0540\n",
      "Validation \tLoss_D: 1.3585\tLoss_G: 0.8684 \tMSE_loss: 3.7476 \tBest_loss: 3.7341\n",
      "[151/200][141/142]\tLoss_D: 1.0910\tLoss_G: 1.0247 \tMSE_loss: 4.0908 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.3479\tLoss_G: 0.8642 \tMSE_loss: 3.7798 \tBest_loss: 3.7341\n",
      "[152/200][141/142]\tLoss_D: 1.0235\tLoss_G: 1.0141 \tMSE_loss: 3.7324 \tTime: 0.0506\n",
      "Validation \tLoss_D: 1.3466\tLoss_G: 0.8626 \tMSE_loss: 3.7778 \tBest_loss: 3.7341\n",
      "[153/200][141/142]\tLoss_D: 1.1034\tLoss_G: 1.0313 \tMSE_loss: 5.1348 \tTime: 0.0430\n",
      "Validation \tLoss_D: 1.3651\tLoss_G: 0.8509 \tMSE_loss: 3.7803 \tBest_loss: 3.7341\n",
      "[154/200][141/142]\tLoss_D: 1.2548\tLoss_G: 1.0332 \tMSE_loss: 4.7594 \tTime: 0.0449\n",
      "Validation \tLoss_D: 1.3637\tLoss_G: 0.8612 \tMSE_loss: 3.7710 \tBest_loss: 3.7341\n",
      "[155/200][141/142]\tLoss_D: 1.1664\tLoss_G: 1.0762 \tMSE_loss: 4.4833 \tTime: 0.0428\n",
      "Validation \tLoss_D: 1.3515\tLoss_G: 0.8674 \tMSE_loss: 3.7533 \tBest_loss: 3.7341\n",
      "[156/200][141/142]\tLoss_D: 1.1230\tLoss_G: 1.0995 \tMSE_loss: 4.2292 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.3485\tLoss_G: 0.8670 \tMSE_loss: 3.7609 \tBest_loss: 3.7341\n",
      "[157/200][141/142]\tLoss_D: 1.2034\tLoss_G: 1.1669 \tMSE_loss: 6.5348 \tTime: 0.0462\n",
      "Validation \tLoss_D: 1.3343\tLoss_G: 0.8817 \tMSE_loss: 3.8434 \tBest_loss: 3.7341\n",
      "[158/200][141/142]\tLoss_D: 1.2896\tLoss_G: 1.0147 \tMSE_loss: 4.6076 \tTime: 0.0398\n",
      "Validation \tLoss_D: 1.3569\tLoss_G: 0.8664 \tMSE_loss: 3.7912 \tBest_loss: 3.7341\n",
      "[159/200][141/142]\tLoss_D: 1.1956\tLoss_G: 0.9916 \tMSE_loss: 3.7138 \tTime: 0.0491\n",
      "Validation \tLoss_D: 1.3641\tLoss_G: 0.8485 \tMSE_loss: 3.7704 \tBest_loss: 3.7341\n",
      "[160/200][141/142]\tLoss_D: 1.1022\tLoss_G: 1.2737 \tMSE_loss: 7.1846 \tTime: 0.0462\n",
      "Validation \tLoss_D: 1.3585\tLoss_G: 0.8569 \tMSE_loss: 3.8275 \tBest_loss: 3.7341\n",
      "[161/200][141/142]\tLoss_D: 1.2428\tLoss_G: 1.0522 \tMSE_loss: 3.7597 \tTime: 0.0499\n",
      "Validation \tLoss_D: 1.3690\tLoss_G: 0.8442 \tMSE_loss: 3.7760 \tBest_loss: 3.7341\n",
      "[162/200][141/142]\tLoss_D: 1.2709\tLoss_G: 1.1129 \tMSE_loss: 5.1653 \tTime: 0.0524\n",
      "Validation \tLoss_D: 1.3668\tLoss_G: 0.8571 \tMSE_loss: 3.7430 \tBest_loss: 3.7341\n",
      "[163/200][141/142]\tLoss_D: 1.1972\tLoss_G: 0.9229 \tMSE_loss: 5.0486 \tTime: 0.0402\n",
      "Validation \tLoss_D: 1.3688\tLoss_G: 0.8447 \tMSE_loss: 3.7565 \tBest_loss: 3.7341\n",
      "[164/200][141/142]\tLoss_D: 1.2684\tLoss_G: 0.9129 \tMSE_loss: 3.1005 \tTime: 0.0434\n",
      "Validation \tLoss_D: 1.3693\tLoss_G: 0.8462 \tMSE_loss: 3.7931 \tBest_loss: 3.7341\n",
      "[165/200][141/142]\tLoss_D: 1.0978\tLoss_G: 1.1662 \tMSE_loss: 5.1139 \tTime: 0.0443\n",
      "Validation \tLoss_D: 1.3639\tLoss_G: 0.8597 \tMSE_loss: 3.8389 \tBest_loss: 3.7341\n",
      "[166/200][141/142]\tLoss_D: 1.2216\tLoss_G: 1.1389 \tMSE_loss: 4.2886 \tTime: 0.0488\n",
      "Validation \tLoss_D: 1.3792\tLoss_G: 0.8481 \tMSE_loss: 3.7919 \tBest_loss: 3.7341\n",
      "[167/200][141/142]\tLoss_D: 1.1095\tLoss_G: 0.9435 \tMSE_loss: 4.5183 \tTime: 0.0546\n",
      "Validation \tLoss_D: 1.3726\tLoss_G: 0.8615 \tMSE_loss: 3.7652 \tBest_loss: 3.7341\n",
      "[168/200][141/142]\tLoss_D: 1.2190\tLoss_G: 0.9478 \tMSE_loss: 3.9893 \tTime: 0.0493\n",
      "Validation \tLoss_D: 1.3614\tLoss_G: 0.8674 \tMSE_loss: 3.7998 \tBest_loss: 3.7341\n",
      "[169/200][141/142]\tLoss_D: 1.1209\tLoss_G: 1.0854 \tMSE_loss: 3.5701 \tTime: 0.0398\n",
      "Validation \tLoss_D: 1.3545\tLoss_G: 0.8632 \tMSE_loss: 3.7982 \tBest_loss: 3.7341\n",
      "[170/200][141/142]\tLoss_D: 1.1934\tLoss_G: 0.8840 \tMSE_loss: 2.5400 \tTime: 0.0361\n",
      "Validation \tLoss_D: 1.3798\tLoss_G: 0.8446 \tMSE_loss: 3.7693 \tBest_loss: 3.7341\n",
      "[171/200][141/142]\tLoss_D: 1.1386\tLoss_G: 1.1437 \tMSE_loss: 5.7819 \tTime: 0.0511\n",
      "Validation \tLoss_D: 1.3807\tLoss_G: 0.8510 \tMSE_loss: 3.7552 \tBest_loss: 3.7341\n",
      "[172/200][141/142]\tLoss_D: 1.1431\tLoss_G: 1.0133 \tMSE_loss: 5.4775 \tTime: 0.0376\n",
      "Validation \tLoss_D: 1.3818\tLoss_G: 0.8422 \tMSE_loss: 3.7787 \tBest_loss: 3.7341\n",
      "[173/200][141/142]\tLoss_D: 1.4048\tLoss_G: 0.8701 \tMSE_loss: 3.8847 \tTime: 0.0434\n",
      "Validation \tLoss_D: 1.3758\tLoss_G: 0.8344 \tMSE_loss: 3.7683 \tBest_loss: 3.7341\n",
      "[174/200][141/142]\tLoss_D: 1.3967\tLoss_G: 0.8188 \tMSE_loss: 2.6593 \tTime: 0.0499\n",
      "Validation \tLoss_D: 1.3842\tLoss_G: 0.8356 \tMSE_loss: 3.7670 \tBest_loss: 3.7341\n",
      "[175/200][141/142]\tLoss_D: 1.1586\tLoss_G: 1.0427 \tMSE_loss: 4.9146 \tTime: 0.0440\n",
      "Validation \tLoss_D: 1.3779\tLoss_G: 0.8432 \tMSE_loss: 3.8050 \tBest_loss: 3.7341\n",
      "[176/200][141/142]\tLoss_D: 0.9974\tLoss_G: 1.1303 \tMSE_loss: 5.0728 \tTime: 0.0421\n",
      "Validation \tLoss_D: 1.3461\tLoss_G: 0.8749 \tMSE_loss: 3.8587 \tBest_loss: 3.7341\n",
      "[177/200][141/142]\tLoss_D: 1.1431\tLoss_G: 1.0752 \tMSE_loss: 3.9724 \tTime: 0.0524\n",
      "Validation \tLoss_D: 1.3832\tLoss_G: 0.8313 \tMSE_loss: 3.7647 \tBest_loss: 3.7341\n",
      "[178/200][141/142]\tLoss_D: 1.3478\tLoss_G: 0.9285 \tMSE_loss: 5.2424 \tTime: 0.0475\n",
      "Validation \tLoss_D: 1.3801\tLoss_G: 0.8425 \tMSE_loss: 3.7508 \tBest_loss: 3.7341\n",
      "[179/200][141/142]\tLoss_D: 1.1949\tLoss_G: 0.8649 \tMSE_loss: 5.4187 \tTime: 0.0431\n",
      "Validation \tLoss_D: 1.3870\tLoss_G: 0.8435 \tMSE_loss: 3.7499 \tBest_loss: 3.7341\n",
      "[180/200][141/142]\tLoss_D: 1.1275\tLoss_G: 1.1736 \tMSE_loss: 3.9667 \tTime: 0.0472\n",
      "Validation \tLoss_D: 1.3815\tLoss_G: 0.8575 \tMSE_loss: 3.8186 \tBest_loss: 3.7341\n",
      "[181/200][141/142]\tLoss_D: 1.2896\tLoss_G: 1.1192 \tMSE_loss: 6.6146 \tTime: 0.0478\n",
      "Validation \tLoss_D: 1.3829\tLoss_G: 0.8500 \tMSE_loss: 3.7887 \tBest_loss: 3.7341\n",
      "[182/200][141/142]\tLoss_D: 1.1222\tLoss_G: 0.7589 \tMSE_loss: 3.2406 \tTime: 0.0461\n",
      "Validation \tLoss_D: 1.3905\tLoss_G: 0.8408 \tMSE_loss: 3.7700 \tBest_loss: 3.7341\n",
      "[183/200][141/142]\tLoss_D: 1.2389\tLoss_G: 0.9077 \tMSE_loss: 5.1926 \tTime: 0.0408\n",
      "Validation \tLoss_D: 1.3842\tLoss_G: 0.8493 \tMSE_loss: 3.7749 \tBest_loss: 3.7341\n",
      "[184/200][141/142]\tLoss_D: 1.2654\tLoss_G: 1.0025 \tMSE_loss: 3.2545 \tTime: 0.0450\n",
      "Validation \tLoss_D: 1.3893\tLoss_G: 0.8379 \tMSE_loss: 3.7432 \tBest_loss: 3.7341\n",
      "[185/200][141/142]\tLoss_D: 1.4229\tLoss_G: 1.0877 \tMSE_loss: 4.4329 \tTime: 0.0388\n",
      "Validation \tLoss_D: 1.3762\tLoss_G: 0.8452 \tMSE_loss: 3.7986 \tBest_loss: 3.7341\n",
      "[186/200][141/142]\tLoss_D: 1.2250\tLoss_G: 0.8895 \tMSE_loss: 3.8347 \tTime: 0.0544\n",
      "Validation \tLoss_D: 1.3882\tLoss_G: 0.8373 \tMSE_loss: 3.7618 \tBest_loss: 3.7341\n",
      "[187/200][141/142]\tLoss_D: 1.3900\tLoss_G: 1.0581 \tMSE_loss: 5.4423 \tTime: 0.0466\n",
      "Validation \tLoss_D: 1.3875\tLoss_G: 0.8412 \tMSE_loss: 3.7991 \tBest_loss: 3.7341\n",
      "[188/200][141/142]\tLoss_D: 1.0775\tLoss_G: 1.0334 \tMSE_loss: 4.3886 \tTime: 0.0402\n",
      "Validation \tLoss_D: 1.3882\tLoss_G: 0.8372 \tMSE_loss: 3.7988 \tBest_loss: 3.7341\n",
      "[189/200][141/142]\tLoss_D: 1.2643\tLoss_G: 0.9575 \tMSE_loss: 5.0958 \tTime: 0.0400\n",
      "Validation \tLoss_D: 1.3974\tLoss_G: 0.8322 \tMSE_loss: 3.7800 \tBest_loss: 3.7341\n",
      "[190/200][141/142]\tLoss_D: 1.1051\tLoss_G: 0.9553 \tMSE_loss: 4.9918 \tTime: 0.0534\n",
      "Validation \tLoss_D: 1.3950\tLoss_G: 0.8334 \tMSE_loss: 3.7591 \tBest_loss: 3.7341\n",
      "[191/200][141/142]\tLoss_D: 1.1506\tLoss_G: 1.2909 \tMSE_loss: 4.8729 \tTime: 0.0470\n",
      "Validation \tLoss_D: 1.3918\tLoss_G: 0.8356 \tMSE_loss: 3.7835 \tBest_loss: 3.7341\n",
      "[192/200][141/142]\tLoss_D: 1.1166\tLoss_G: 1.0626 \tMSE_loss: 4.0961 \tTime: 0.0471\n",
      "Validation \tLoss_D: 1.4031\tLoss_G: 0.8302 \tMSE_loss: 3.7728 \tBest_loss: 3.7341\n",
      "[193/200][141/142]\tLoss_D: 0.9869\tLoss_G: 1.1770 \tMSE_loss: 7.3052 \tTime: 0.0412\n",
      "Validation \tLoss_D: 1.4013\tLoss_G: 0.8295 \tMSE_loss: 3.7736 \tBest_loss: 3.7341\n",
      "[194/200][141/142]\tLoss_D: 1.0855\tLoss_G: 1.0282 \tMSE_loss: 4.6369 \tTime: 0.0454\n",
      "Validation \tLoss_D: 1.4038\tLoss_G: 0.8302 \tMSE_loss: 3.7804 \tBest_loss: 3.7341\n",
      "[195/200][141/142]\tLoss_D: 1.2550\tLoss_G: 0.9010 \tMSE_loss: 2.8837 \tTime: 0.0447\n",
      "Validation \tLoss_D: 1.3982\tLoss_G: 0.8333 \tMSE_loss: 3.7675 \tBest_loss: 3.7341\n",
      "[196/200][141/142]\tLoss_D: 1.2575\tLoss_G: 1.1321 \tMSE_loss: 3.9096 \tTime: 0.0425\n",
      "Validation \tLoss_D: 1.4015\tLoss_G: 0.8307 \tMSE_loss: 3.7800 \tBest_loss: 3.7341\n",
      "[197/200][141/142]\tLoss_D: 1.2399\tLoss_G: 0.9965 \tMSE_loss: 3.8036 \tTime: 0.0502\n",
      "Validation \tLoss_D: 1.4108\tLoss_G: 0.8281 \tMSE_loss: 3.7540 \tBest_loss: 3.7341\n",
      "[198/200][141/142]\tLoss_D: 1.2883\tLoss_G: 0.9594 \tMSE_loss: 5.6584 \tTime: 0.0404\n",
      "Validation \tLoss_D: 1.4111\tLoss_G: 0.8194 \tMSE_loss: 3.7821 \tBest_loss: 3.7341\n",
      "[199/200][141/142]\tLoss_D: 1.0964\tLoss_G: 0.9973 \tMSE_loss: 4.0603 \tTime: 0.0475\n",
      "Validation \tLoss_D: 1.4023\tLoss_G: 0.8287 \tMSE_loss: 3.7970 \tBest_loss: 3.7341\n"
     ]
    }
   ],
   "source": [
    "results = train_GAN(generator, discriminator, generator_optimizer, discriminator_optimizer, loss_fn, data_gen_train, data_gen_test, epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad79201f-0488-439d-8b3b-59bae2a4687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE =  tensor(3.10181688, device='cuda:0', dtype=torch.float64)\n",
      "RMSE =  tensor(2.02099880, device='cuda:0', dtype=torch.float64)\n",
      "MAE =  tensor(1.33180923, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load('lstm_gan.pth')['model_state_dict'])\n",
    "data = dataframe.drop(columns='Date').to_numpy()\n",
    "targets = data\n",
    "n_samples = data.shape[0]\n",
    "train_test_split=int(n_samples*0.9)\n",
    "train_data = data[:train_test_split]\n",
    "test_data = data[train_test_split:]\n",
    "train_target = targets[:train_test_split]\n",
    "test_target = targets[train_test_split:]\n",
    "test_data=Standarized_TimeseriesGenerator(test_data, test_target, length=n_sequence, stride=1)\n",
    "\n",
    "mape = 0\n",
    "rmse = 0\n",
    "mae = 0\n",
    "\n",
    "for i in range(0,len(test_data)-1,2):\n",
    "    with torch.no_grad():\n",
    "        seq = torch.concat((test_data[i][0].reshape(1,5,7),test_data[i+1][0].reshape(1,5,7)),dim=0)\n",
    "        targets = torch.concat((test_data[i][1].reshape(1,1,7),test_data[i+1][1].reshape(1,1,7)),dim=0).to(device)\n",
    "        pred = generator(seq.to(device))\n",
    "        mape += torch.sum(torch.abs(targets[:,:,3] - pred[:,:,3] /targets[:,:,3]) )\n",
    "        rmse += torch.sum((targets[:,:,3]  - pred[:,:,3] )**2)\n",
    "        mae += torch.sum(torch.abs(targets[:,:,3] - pred[:,:,3] ))\n",
    "        \n",
    "print(\"MAPE = \",mape/len(test_data))\n",
    "print(\"RMSE = \",(rmse/len(test_data))**0.5)\n",
    "print(\"MAE = \",mae/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6f49f-b976-4b86-a794-70f3fda27b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
